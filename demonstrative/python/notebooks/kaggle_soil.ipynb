{
 "metadata": {
  "name": "",
  "signature": "sha256:08c1cb03d5f2165f14f5b24ae7b3d3642a036c045cf2fab5d45ae340d38502f0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from matplotlib import pyplot as plt\n",
      "import copy as cp\n",
      "np.random.seed(123)\n",
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Dataset:\n",
      "    _cols_y = ['Ca', 'P', 'pH', 'SOC', 'Sand']\n",
      "    _cols_other = ['CTI','ELEV','EVI','LSTD','LSTN','RELI','TMAP','TMFI','Depth']\n",
      "    \n",
      "    def __init__(self, f='/Users/eczech/kaggle/soil/training.csv', sample_rate=.5):\n",
      "        self.data = pd.io.parsers.read_csv(f)\n",
      "        self.data['Depth'] = self.data['Depth'].map({'Topsoil': 0, 'Subsoil': 1})\n",
      "        if sample_rate:\n",
      "            n = len(self.data)\n",
      "            self.data = self.data.iloc[np.random.permutation(n)[:int(.5*n)]]\n",
      "        self.data.set_index('PIDN', inplace=True)\n",
      "        self.N = len(self.data)\n",
      "        self.cols = self.data.columns.tolist()\n",
      "        \n",
      "    def X_IR(self):\n",
      "        return self.data[[col for col in self.cols if col.startswith('m')]]\n",
      "    \n",
      "    def X_nonPC(self):\n",
      "        return self.data[[col for col in self.X().columns.tolist() if not col.startswith('pc_') and col != 'Depth']]\n",
      "    \n",
      "    def X_BSA(self):\n",
      "        return self.data[[col for col in self.cols if col.startswith('BSA')]]\n",
      "    \n",
      "    def X_REF(self):\n",
      "        return self.data[[col for col in self.cols if col.startswith('REF')]]\n",
      "    \n",
      "    def X(self):\n",
      "        return self.data[[col for col in self.cols if not col in self._cols_y]]\n",
      "    \n",
      "    def Y(self):\n",
      "        return self.data[self._cols_y]\n",
      "    \n",
      "dataset = Dataset()\n",
      "dataset.data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Feature Extraction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This function will be used to decompose highly collinear features via PCA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def redux_features(dataset, features, prefix, n_components=.99, show_exp_variance=False):\n",
      "    dataset = cp.deepcopy(dataset)\n",
      "    from sklearn.preprocessing import scale\n",
      "    from sklearn import decomposition\n",
      "    \n",
      "    print 'Running PCA to collapse given features {}'.format(\n",
      "        features if len(features) <= 10 else features[:5] + ['...'] + features[-5:] \n",
      "    )\n",
      "    X = scale(dataset.X()[features])\n",
      "    n_cols = X.shape[1]\n",
      "    pca = decomposition.PCA(n_components=n_components).fit(X)\n",
      "    X = pca.transform(X)\n",
      "    redux_n_cols = X.shape[1]\n",
      "    redux_features = [prefix + str(i) for i in range(redux_n_cols)]\n",
      "    print '{} features reduced to {} primary components (with feature names prefixed by \"{}\")'.format(n_cols, redux_n_cols, prefix)\n",
      "    if show_exp_variance:\n",
      "        vals = pca.explained_variance_ratio_[:redux_n_cols]\n",
      "        idx = redux_features\n",
      "        pd.Series(vals, index=idx).plot(kind='bar', figsize=(12,4), title='Explained Variance')\n",
      "    \n",
      "\n",
      "    ix = np.arange(dataset.N)\n",
      "    data = pd.concat([\n",
      "        dataset.data.drop(features, axis=1).set_index(ix), \n",
      "        pd.DataFrame(X, columns=redux_features).set_index(ix)\n",
      "    ], axis=1, ignore_index=False)\n",
      "    \n",
      "    dataset.data = data\n",
      "    dataset.cols = data.columns.tolist()\n",
      "    return dataset"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Decomposing IR sample values "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, reduce the 3578 wavelength features to something more manageable.\n",
      "\n",
      "To do this, the PCA compression function above will be used to reduce those 3k+ features to the ~10 orthogonal dimensions necessary to explain > 99% of their variation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset = redux_features(dataset, dataset.X_IR().columns.tolist(), 'pc_m')\n",
      "dataset.data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Preliminary Feature Importance Tests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before going further, take a look at the importance of the features we currently have.  This should be done first to see if there are any likely interactions or nonlinearities that need to be accounted for before assuming that certain features can be combined."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_feature_importances(dataset):\n",
      "    \"\"\"Returns a data frame containing the relative importances \n",
      "    of each feature for each dependent variable\n",
      "    \"\"\"\n",
      "    from sklearn.ensemble import ExtraTreesRegressor\n",
      "\n",
      "    res = pd.DataFrame()\n",
      "    for response in dataset.Y().columns.tolist():\n",
      "        X, y = dataset.X(), dataset.data[response]\n",
      "\n",
      "        clf = ExtraTreesRegressor(n_estimators=100)    \n",
      "        clf.fit(X, y)\n",
      "\n",
      "        res[response] = pd.Series(clf.feature_importances_, index=X.columns.tolist())\n",
      "    return res"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = get_feature_importances(dataset)\n",
      "features.plot(kind='bar', figsize=(16, 4))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apparently the IR wavelength values are going to be the most important predictors as well as _ELEV_, _TMAP_, _TMFI_, _RELI_ and possibly some of the _REF*_ features.  \n",
      "\n",
      "Given the definition of the _REF*_ features as well as the _BSA*_ features, it is possible that they will be highly correlated and might produce a more useful feature if decomposed.  Those two feature groups may be independently correlated with one another or they may all be indicative of the same thing (along with other features for that matter too).  \n",
      "\n",
      "Before combining any features further, let's see how well the ones named above correlate to one another on a linear basis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First, look for linear correlations amongst remaining features\n",
      "def plot_correlations(data):\n",
      "    from matplotlib import cm\n",
      "    cols = data.columns.tolist()\n",
      "    fig = plt.figure(figsize=(12,12))\n",
      "    ax = fig.add_subplot(111)\n",
      "    \n",
      "    # Plot absolute value of pairwise correlations since we don't\n",
      "    # particularly care about the direction of the relationship yet\n",
      "    cax = ax.matshow(data.corr().abs(), cmap=cm.YlOrRd)\n",
      "    \n",
      "    fig.colorbar(cax)\n",
      "    ax.set_xticks(np.arange(len(cols)))\n",
      "    ax.set_yticks(np.arange(len(cols)))\n",
      "    ax.set_xticklabels(cols)\n",
      "    ax.set_yticklabels(cols)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_correlations(dataset.X_nonPC())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a lot to take in but at the very least it's clear that the _REF*_ and _BSA*_ features are all highly correlated.\n",
      "\n",
      "Like the initial IR wavelength features we'll combine these to create a smaller overall feature set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compress REF* and BSA* features into a smaller dimensional space\n",
      "redux_cols = dataset.X_REF().columns.tolist() + dataset.X_BSA().columns.tolist()\n",
      "dataset = redux_features(dataset, redux_cols, 'pc_b', n_components=.9)\n",
      "dataset.data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we've reduced the 7 features named above down to approximately 2, which should give something that can be visualized in greater detail.\n",
      "\n",
      "To do that, look at pairwise scatter plots of each of the remaining features to see if any more features should be extracted."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas.tools.plotting import scatter_matrix\n",
      "scatter_matrix(dataset.X_nonPC(), figsize=(12,12), diagonal='kde')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Most features don't correlate well together but it appears there are still largely linear relationships between _TMAP_ + _TMFI_ and _LSTN_ + _ELEV_.  These could be combined through a single decomposition but we'll do them separately instead so that it's clear what the resulting features correspond to."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = dataset.X().copy()\n",
      "x['CTI'] = x.CTI.apply(lambda x: math.asinh(x))\n",
      "#scatter_matrix(x, figsize=(12,12), diagonal='kde')\n",
      "#x.CTI.plot(kind='histogram')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Extract features from the following pairs\n",
      "dataset = redux_features(dataset, ['TMAP', 'TMFI'], 'pc_t', n_components=1)\n",
      "dataset = redux_features(dataset, ['LSTN', 'ELEV'], 'pc_e', n_components=1)\n",
      "\n",
      "# Print the final feature set\n",
      "dataset.X().columns.tolist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At this point, these features have been extracted as the final set:\n",
      "- P, pH, SOC, Sand - Values to predict\n",
      "- LSTD, EVI, Depth, RELI - Orignal, unaltered features\n",
      "- pc_m{0..8} - Principal components for 3,578 original IR absortion levels\n",
      "- pc_b{0..1} - Principal components for 11 original _REF*_ and _BSA*_ features\n",
      "- pc_t0 - Principal components for _TMAP_ and _TMFI_\n",
      "- pc_e0 - Principal components for _LSTN_ and _ELEV_\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Feature Selection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "def select_features(dataset, selector):\n",
      "    #ct = Counter(dataset.X().columns.tolist())\n",
      "    #ct.subtract(list(ct))\n",
      "    res = {}\n",
      "    for response in dataset.Y().columns.tolist():\n",
      "        X, y = dataset.X(), dataset.Y()[response]\n",
      "        features = selector(X, y)\n",
      "        for col in X.columns.tolist():\n",
      "            if not col in features.keys():\n",
      "                features[col] = 0\n",
      "        #ct.update(features)\n",
      "        res[response] = features\n",
      "    return pd.DataFrame(res).transpose().fillna(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "votes.mean(axis=0).order(ascending=False).plot(kind='bar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tree_selector(X, y):\n",
      "    from sklearn.ensemble import ExtraTreesRegressor\n",
      "    \n",
      "    clf = ExtraTreesRegressor(n_estimators=100)\n",
      "    fit = clf.fit(dataset.X(), y)\n",
      "    X_new = fit.transform(dataset.X())\n",
      "    importance = pd.Series(fit.feature_importances_, index=X.columns.tolist()).order(ascending=False)\n",
      "    return importance[:X_new.shape[1]].to_dict()\n",
      "\n",
      "votes = select_features(dataset, tree_selector)\n",
      "votes.mean(axis=0).order(ascending=False).plot(kind='bar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rfe_selector(X, y):\n",
      "    from sklearn.svm import SVR\n",
      "    from sklearn.preprocessing import scale\n",
      "    \n",
      "    selector = RFE(SVR(kernel='linear'), step=1, verbose=1)\n",
      "    selector.fit(scale(X), y)\n",
      "    return pd.Series(selector.ranking_, index=X.columns.tolist()).to_dict()\n",
      "    \n",
      "votes = select_features(dataset, rfe_selector)\n",
      "votes.mean(axis=0).order(ascending=False).plot(kind='bar')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Model Selection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for col in dataset.X().columns.tolist():\n",
      "    dataset.data[[col, 'P']].plot(kind='scatter', x=col, y='P')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = dataset.data.copy()\n",
      "#for _ in range(10):\n",
      "#    d['P'] = d.P.apply(math.asinh)\n",
      "scatter_matrix(d[dataset.X().columns.tolist() + ['P']], figsize=(30,30))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
      "from sklearn.svm import SVR\n",
      "from sklearn.cross_validation import KFold, cross_val_score\n",
      "from sklearn.preprocessing import scale\n",
      "from sklearn.metrics import mean_squared_error\n",
      "\n",
      "cv = KFold(len(dataset.data), n_folds=5)\n",
      "res = []\n",
      "\n",
      "Y = dataset.Y().copy()\n",
      "log_vars = ['SOC', 'Ca']\n",
      "d_log_vars = []\n",
      "#Y['P'] = Y.P.apply(lambda x: math.asinh(x))\n",
      "#Y['SOC'] = Y.SOC.apply(lambda x: math.asinh(x))\n",
      "#Y['Ca'] = Y.Ca.apply(lambda x: math.asinh(x))\n",
      "\n",
      "for train, test in cv:\n",
      "    scores = {}\n",
      "    for response in dataset.Y().columns.tolist():\n",
      "        X, y = dataset.X(), Y[response]\n",
      "            \n",
      "        X['CTI'] = X.CTI.apply(lambda x: math.asinh(x))\n",
      "        \n",
      "        X_train, X_test, y_train, y_test = scale(X.iloc[train]), scale(X.iloc[test]), y.iloc[train], y.iloc[test]\n",
      "        \n",
      "        if response in log_vars:\n",
      "            y_train = y_train.apply(lambda x: math.asinh(x))\n",
      "        if response in d_log_vars:\n",
      "            for _ in range(10):\n",
      "                y_train = y_train.apply(math.asinh)\n",
      "            \n",
      "        clf = GradientBoostingRegressor(n_estimators=1000)\n",
      "        #clf = RandomForestRegressor(n_estimators=100)    \n",
      "        #clf = SVR()    \n",
      "        \n",
      "        fit = clf.fit(X_train, y_train)\n",
      "        y_predict = pd.Series(clf.predict(X_test))\n",
      "        \n",
      "        if response in log_vars:\n",
      "            y_predict = y_predict.apply(lambda x: math.sinh(x))\n",
      "        if response in d_log_vars:\n",
      "            for _ in range(10):\n",
      "                y_predict = y_predict.apply(math.sinh)\n",
      "        if response == 'P':\n",
      "            y_predict.plot(kind='kde')\n",
      "            print pd.qcut(y_predict, 20)\n",
      "        \n",
      "        #print pd.Series(fit.feature_importances_, index=X.columns.tolist()).order(ascending=False)\n",
      "        #scores = CV.cross_val_score(reg, X, y, cv=10, scoring='mean_squared_error')\n",
      "        #cross_val_score(clf, )\n",
      "        #cross_val_score(clf, X_train)\n",
      "        \n",
      "        #scores[response] = clf.score(X_test, y_test)\n",
      "        scores[response] = mean_squared_error(y_test, y_predict)\n",
      "    res.append(scores)\n",
      "rmse = pd.DataFrame(res)\n",
      "rmse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rmse[['Ca', 'SOC', 'Sand', 'pH']].mean(axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = dataset.Y().copy()\n",
      "x['P'] = x.P.apply(np.log)\n",
      "x['SOC'] = x.SOC.apply(np.log)\n",
      "x['Ca'] = x.Ca.apply(np.log)\n",
      "scatter_matrix(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Finding relevant features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Visualizing the relationship between features and dependent variables"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dataset.data[dataset.X_BSA().columns.tolist() + dataset.Y().columns.tolist()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas.tools.plotting import scatter_matrix\n",
      "scatter_matrix(dataset.data[dataset.X_BSA().columns.tolist() + dataset.Y().columns.tolist()], figsize=(20,20))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finding features using RFE with regularized regression"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_important_features(dataset):\n",
      "    from sklearn.ensemble import ExtraTreesRegressor\n",
      "\n",
      "    res = pd.DataFrame()\n",
      "    for response in dataset.cols_y:\n",
      "        X, y = dataset.X(), dataset.data[response]\n",
      "\n",
      "        clf = ExtraTreesRegressor(n_estimators=100)    \n",
      "        clf.fit(X, y)\n",
      "        X_new = clf.transform(X)\n",
      "\n",
      "        #print response, X_new.shape, len(clf.estimators_)\n",
      "        res[response] = pd.Series(clf.feature_importances_, index=X.columns.tolist())\n",
      "    return res.mean(axis=1).order(ascending=False)\n",
      "#print res\n",
      "features = get_important_features(dataset)\n",
      "features.plot(kind='bar', figsize=(20, 6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finding features using tree ensembles"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_important_features(dataset):\n",
      "    from sklearn.ensemble import ExtraTreesRegressor\n",
      "\n",
      "    res = pd.DataFrame()\n",
      "    for response in dataset.cols_y:\n",
      "        X, y = dataset.X(), dataset.data[response]\n",
      "\n",
      "        clf = ExtraTreesRegressor(n_estimators=100)    \n",
      "        clf.fit(X, y)\n",
      "        X_new = clf.transform(X)\n",
      "\n",
      "        #print response, X_new.shape, len(clf.estimators_)\n",
      "        res[response] = pd.Series(clf.feature_importances_, index=X.columns.tolist())\n",
      "    return res.mean(axis=1).order(ascending=False)\n",
      "#print res\n",
      "features = get_important_features(dataset)\n",
      "features.plot(kind='bar', figsize=(20, 6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas.tools.plotting import scatter_matrix\n",
      "scatter_matrix(dataset.data[features.index.values])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# MDS clustering for dependent variables\n",
      "\n",
      "from sklearn import manifold\n",
      "\n",
      "mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=rand_seed, dissimilarity=\"euclidean\", n_jobs=1)\n",
      "mds.fit(ir_redux_data[cols_dep])\n",
      "pd.DataFrame(mds.embedding_).plot(kind='scatter', x=0, y=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Dependent Variable Exploration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "####Looking For Clusters"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cluster import KMeans\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "X = scale(data[cols_dep])\n",
      "\n",
      "# Compute clusters via dbscan\n",
      "clusters = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
      "\n",
      "# Run MDS for plotting\n",
      "mds = manifold.MDS(n_components=2, random_state=rand_seed, dissimilarity=\"euclidean\", n_jobs=-1).fit(X).embedding_\n",
      "fig = plt.figure()\n",
      "ax = fig.gca()\n",
      "ax.scatter(mds[:,0], mds[:,1], c=clusters.labels_.astype(np.float))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#####Print the points furthest from the origin via MDS axes for comparison to Mahalanobis distance"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame(mds).apply(np.linalg.norm, axis=1).order(ascending=False)[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Mahalanobis Distance for outlier detection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.covariance import EmpiricalCovariance\n",
      "from sklearn.preprocessing import scale\n",
      "\n",
      "X = scale(data[cols_dep])\n",
      "# Compute distances and print furthest 10 points for comparison to above\n",
      "dist = EmpiricalCovariance(store_precision=True).fit(X).mahalanobis(X)\n",
      "pd.Series(dist).order(ascending=False)[:10]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###IR Sample Clustering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas.tools.plotting import scatter_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import manifold\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "X = data[cols_ir]\n",
      "#X_cor = X.transpose().corr()\n",
      "\n",
      "clusters = KMeans(n_clusters=5).fit(X)\n",
      "mds = manifold\\\n",
      "    .MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=rand_seed, dissimilarity=\"euclidean\", n_jobs=-11)\\\n",
      "    .fit(X).embedding_\n",
      "plt.scatter(mds[:,0], mds[:,1], c=clusters.labels_.astype(np.float))\n",
      "#pd.DataFrame(mds.embedding_).plot(kind='scatter', x=0, y=1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#min_val = corrs.apply(np.min).min()\n",
      "#corrs = corrs.applymap(lambda x: (x - min_val) / (1 - min_val))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import manifold\n",
      "mds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=rand_seed, dissimilarity=\"precomputed\", n_jobs=1)\n",
      "mds_fit = mds.fit(corrs)\n",
      "#mds_fit = mds.fit(data[ir_samples])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.DataFrame(mds.embedding_).plot(kind='scatter', x=0, y=1) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}