{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_printv(t, transform=None):\n",
    "    def log_value(x):\n",
    "        if transform is not None:\n",
    "            v = transform(x)\n",
    "            if v is not None:\n",
    "                logger.info('{} - {}'.format(t.name, v))\n",
    "        else:\n",
    "            logger.info('{} - {}'.format(t.name, x))\n",
    "        return x\n",
    "    log_op = tf.py_func(log_value, [t], [t.dtype], name=t.name.split(':')[0])[0]\n",
    "    with tf.control_dependencies([log_op]):\n",
    "        r = tf.identity(t)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Inflated Count Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Normal, Laplace, PointMass, NegativeBinomial, Bernoulli, Poisson\n",
    "from ml.tensorflow.utilities import tf_print\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Example of using shifted poisson as positive distribution in hurdle model:\n",
    "# https://www.casact.org/education/annual/2011/handouts/P2-Boucher.pdf\n",
    "\n",
    "class ModelBuilder(object):\n",
    "    \n",
    "    def __init__(self, inference_fn):\n",
    "        self.map = inference_fn == ed.MAP\n",
    "        self.latent_map = {}\n",
    "        self.tensor_map = {}\n",
    "        \n",
    "        \n",
    "    def add(self, dist, loc, scale, name, loc_transform=tf.identity, scale_transform=tf.nn.softplus,\n",
    "           scale_coef=.1):\n",
    "        shape = loc.get_shape().as_list()\n",
    "        model = dist(loc, scale * tf.ones_like(loc))\n",
    "\n",
    "        lm = self.latent_map\n",
    "        tm = self.tensor_map\n",
    "        if self.map:\n",
    "            q = PointMass(params=loc_transform(tf.Variable(tf.random_normal(shape, stddev=scale*scale_coef))))\n",
    "            lm[model] = q\n",
    "            tm[name] = model\n",
    "            tm[name + '.q'] = q.params\n",
    "            #tf.summary.histogram(name + '.q', tm[name + '.q'])\n",
    "        else:\n",
    "            q = dist(\n",
    "                loc_transform(tf.Variable(tf.random_normal(shape, stddev=scale*scale_coef))), \n",
    "                scale_transform(tf.Variable(tf.random_normal(shape, stddev=scale*scale_coef)))\n",
    "            )\n",
    "            lm[model] = q\n",
    "            tm[name] = model\n",
    "            if dist in [Normal, Laplace]:\n",
    "                tm[name + '.q'] = q.loc\n",
    "                tm[name + '.s'] = q.scale\n",
    "            else:\n",
    "                raise ValueError('Distribution \"{}\" not yet supported'.format(dist))\n",
    "            #tf.summary.histogram(name + '.q', tm[name + '.q'])\n",
    "            #tf.summary.histogram(name + '.s', tm[name + '.s'])\n",
    "\n",
    "\n",
    "class CountModel(ed_models.BayesianModel):\n",
    "    \n",
    "    def __init__(\n",
    "        self, inference_fn, \n",
    "        x_thresh=100., y_thresh=None,\n",
    "        ys_max=10., u_sat=10.,\n",
    "        link='nbinom'\n",
    "    ):\n",
    "        self.x_thresh = x_thresh\n",
    "        self.y_thresh = y_thresh\n",
    "        self.ys_max = ys_max\n",
    "        self.u_sat = u_sat\n",
    "        assert link in ['poisson', 'nbinom'], 'Link must be either \"nbinom\" or \"poisson\"'\n",
    "        self.link = link\n",
    "        \n",
    "        self.u_sat_ = None\n",
    "        self.group_encoder_ = None\n",
    "        self.model = ModelBuilder(inference_fn)\n",
    "        \n",
    "    def value_link_inverse(self, v):\n",
    "        return tf.exp(self.u_sat * tf.nn.tanh(v / self.u_sat))\n",
    "        #return tf.exp(v)\n",
    "    \n",
    "    def add(self, *args):\n",
    "        self.builder.add(*args)\n",
    "        \n",
    "    def set_params(self, y):\n",
    "        assert np.all(y >= 0)\n",
    "        y_max = y.max()\n",
    "        self.u_sat_ = np.log(y_max)\n",
    "    \n",
    "    def inference_args(self, data, groups):\n",
    "        \n",
    "        tm, lm = {}, {}\n",
    "        \n",
    "        # Extract true X and Y values, clipping X values to be\n",
    "        # <= self.x_thresh standard deviations (crucial for gradient descent to not give nans)\n",
    "        dX, dY = np.clip(data['X'], -self.x_thresh, self.x_thresh), data['Y']\n",
    "        tm['dX'] = dX\n",
    "        self.set_params(dY)\n",
    "        \n",
    "        assert groups is not None\n",
    "        assert len(groups) == dX.shape[0]\n",
    "        nG = len(np.unique(groups))\n",
    "        self.group_encoder_ = LabelEncoder().fit(groups)\n",
    "        dG = self.group_encoder_.transform(groups).astype(np.int32)\n",
    "    \n",
    "        # Placeholders\n",
    "        assert np.all(dY >= 0.)\n",
    "        P = dX.shape[1]\n",
    "        Xp = tf.placeholder(tf.float32, [None, P])\n",
    "        tm['Xp'] = Xp\n",
    "        X = tf.clip_by_value(Xp, -self.x_thresh, self.x_thresh)\n",
    "        G = tf.placeholder(tf.int32, [None])\n",
    "        tm['G'] = G\n",
    "        Yp = tf.placeholder(tf.int32, [None])\n",
    "        Y = tf.identity(Yp)\n",
    "                \n",
    "        # ##### Model Weights #### #\n",
    "        \n",
    "        self.model.add(Normal, tf.zeros([nG]), .0001, 'wgv')\n",
    "        self.model.add(Normal, tf.zeros([P, 1]), 1., 'wv')\n",
    "        self.model.add(Normal, tf.zeros([]), 1., 'wbv')\n",
    "        \n",
    "        tm.update(self.model.tensor_map)\n",
    "        lm.update(self.model.latent_map)\n",
    "        \n",
    "        \n",
    "        # ##### Expectations #### #\n",
    "        \n",
    "        # Count expectation\n",
    "        Yv_mu_link = tf.gather(tm['wgv'], G) + tm['wbv'] + tf.reshape(tf.matmul(X, tm['wv']), [-1])\n",
    "        Yv_mu = self.value_link_inverse(Yv_mu_link)\n",
    "        tf.summary.histogram('Yv_mu', tf.clip_by_value(Yv_mu, -10000., 10000.))\n",
    "        \n",
    "        \n",
    "        # ##### Error Family Mapping #### #\n",
    "        # Count value error distribution\n",
    "        if self.link == 'nbinom':\n",
    "            tm['Ys'] = tf.cast(self.ys_max * tf.exp(tf.Variable(np.log(1./self.ys_max))), tf.float32)\n",
    "            #tm['Ys'] = 1.\n",
    "            tf.summary.scalar('Ys', tm['Ys'])\n",
    "            cYv = NegativeBinomial(tm['Ys'] * tf.ones_like(Yv_mu), probs=Yv_mu / (Yv_mu + tm['Ys']))\n",
    "        else:\n",
    "            cYv = Poisson(Yv_mu)\n",
    "        \n",
    "        # Sampling\n",
    "        tm['n_samp'] = tf.placeholder(tf.int32)\n",
    "        \n",
    "        tm['Yv_samp'] = ed.copy(cYv, lm).sample(tm['n_samp'])\n",
    "        #tm['Yv_pred'] = ed.copy(Yv_link, lm)\n",
    "        #tm['Yv_prob'] = ed.copy(cYv, lm).log_prob(dY.astype(np.float32))\n",
    "\n",
    "        \n",
    "        def input_fn(d):\n",
    "            return {Xp: dX, Yp: dY, G: dG, cYv: dY}\n",
    "        \n",
    "        return input_fn, lm, tm\n",
    "        \n",
    "    def criticism_args(self, sess, tm):\n",
    "        \n",
    "        def sample_fn(n, X, G):\n",
    "            G = self.group_encoder_.transform(G).astype(np.int32)\n",
    "            Y = sess.run(\n",
    "                tm['Yv_samp'], \n",
    "                feed_dict={tm['Xp']: X, tm['G']: G, tm['n_samp']: n}\n",
    "            )\n",
    "            assert Y.shape == (n, X.shape[0])\n",
    "            \n",
    "            if self.y_thresh is not None:\n",
    "                Y = np.clip(Y, -np.inf, self.y_thresh)\n",
    "            return Y\n",
    "                 \n",
    "        return {\n",
    "            'sample_fn': sample_fn\n",
    "        }\n",
    "    \n",
    "WV = []\n",
    "WZ = []\n",
    "YVMU = []\n",
    "\n",
    "class HurdleCountModel(ed_models.BayesianModel):\n",
    "    \n",
    "    def __init__(\n",
    "        self, inference_fn, \n",
    "        x_thresh=100., y_thresh=None, \n",
    "        p_sat=None, p_sat_max=.3,\n",
    "        ys_max=10., u_sat=10.,\n",
    "        link='nbinom'\n",
    "    ):\n",
    "        self.x_thresh = x_thresh\n",
    "        self.y_thresh = y_thresh\n",
    "        self.p_sat = p_sat\n",
    "        self.p_sat_max = p_sat_max\n",
    "        self.u_sat = u_sat\n",
    "        self.ys_max = ys_max\n",
    "        assert link in ['poisson', 'nbinom'], 'Link must be either \"nbinom\" or \"poisson\"'\n",
    "        self.link = link\n",
    "        \n",
    "        self.u_sat_ = None\n",
    "        self.group_encoder_ = None\n",
    "        self.model = ModelBuilder(inference_fn)\n",
    "        \n",
    "    def zero_link_inverse(self, v, sat):\n",
    "        if sat is not None:\n",
    "            return sat + (1. - 2*sat) * tf.nn.sigmoid(v)\n",
    "        else:\n",
    "            return tf.nn.sigmoid(v)\n",
    "    \n",
    "    def value_link_inverse(self, v):\n",
    "        return tf.exp(self.u_sat * tf.nn.tanh(v / self.u_sat))\n",
    "        #return tf.exp(v)\n",
    "        #return tf.exp(self.u_sat - (tf.nn.softplus(self.u_sat - v)))\n",
    "        #return tf.exp(tf.clip_by_value(v, -self.u_sat, self.u_sat))\n",
    "        #return tf.nn.softplus(tf.clip_by_value(v, -self.u_sat, self.u_sat))\n",
    "    \n",
    "    def add(self, *args):\n",
    "        self.builder.add(*args)\n",
    "        \n",
    "    def set_params(self, y):\n",
    "        assert np.all(y >= 0)\n",
    "        y_max = y.max()\n",
    "        #self.u_sat_ = np.log(y_max)\n",
    "    \n",
    "    def inference_args(self, data, groups):\n",
    "        \n",
    "        tm, lm = {}, {}\n",
    "        \n",
    "        # Extract true X and Y values, clipping X values to be\n",
    "        # <= self.x_thresh standard deviations (crucial for gradient descent to not give nans)\n",
    "        dX, dY = np.clip(data['X'], -self.x_thresh, self.x_thresh), data['Y']\n",
    "        tm['dX'] = dX\n",
    "        self.set_params(dY)\n",
    "        \n",
    "        assert groups is not None\n",
    "        assert len(groups) == dX.shape[0]\n",
    "        nG = len(np.unique(groups))\n",
    "        self.group_encoder_ = LabelEncoder().fit(groups)\n",
    "        dG = self.group_encoder_.transform(groups).astype(np.int32)\n",
    "    \n",
    "        # Separate zeros from positive values\n",
    "        assert np.all(dY >= 0.)\n",
    "        P = dX.shape[1]\n",
    "        Xp = tf.placeholder(tf.float32, [None, P])\n",
    "        tm['Xp'] = Xp\n",
    "        X = tf.clip_by_value(Xp, -self.x_thresh, self.x_thresh)\n",
    "        G = tf.placeholder(tf.int32, [None])\n",
    "        tm['G'] = G\n",
    "        Yp = tf.placeholder(tf.int32, [None])\n",
    "        Y = tf.identity(Yp)\n",
    "        \n",
    "        I = tf.squeeze(tf.where(Y > 0))\n",
    "        #Yv = tf.gather(Y, I)\n",
    "        Xv = tf.gather(X, I)\n",
    "        Yi = tf.cast(Y <= 0, tf.int32)\n",
    "        Gv = tf.gather(G, I)\n",
    "        tf.summary.histogram('I', I)\n",
    "                \n",
    "        # ##### Model Weights #### #\n",
    "        \n",
    "        self.model.add(Normal, tf.zeros([nG]), .0001, 'wgv')\n",
    "        self.model.add(Normal, tf.zeros([nG]), .0001, 'wgz')\n",
    "        self.model.add(Normal, tf.zeros([P, 1]), 1., 'wv')\n",
    "        self.model.add(Normal, tf.zeros([P, 1]), 1., 'wz')\n",
    "        self.model.add(Normal, tf.zeros([]), 10., 'wbv')\n",
    "        self.model.add(Normal, tf.zeros([]), 10., 'wbz')\n",
    "        \n",
    "        tm.update(self.model.tensor_map)\n",
    "        lm.update(self.model.latent_map)\n",
    "        \n",
    "        \n",
    "        def add_vweight(w):\n",
    "            global WV\n",
    "            WV.append(w[:,0])\n",
    "            return None\n",
    "        def add_zweight(w):\n",
    "            global WZ\n",
    "            WZ.append(w[:,0])\n",
    "            return None\n",
    "        #tm['wz'] = tf_printv(tm['wz'], add_zweight)\n",
    "        #tm['wv'] = tf_printv(tm['wv'], add_vweight)\n",
    "        # ##### Expectations #### #\n",
    "        \n",
    "        # Zero expectation\n",
    "        if self.p_sat is None:\n",
    "            Ps = self.p_sat_max * tf.exp(-tf.nn.softplus(tf.Variable(0.)))        \n",
    "        else:\n",
    "            Ps = tf.constant(self.p_sat, tf.float32)\n",
    "            \n",
    "        tf.summary.scalar('Ps', Ps)\n",
    "        Pz_link = tf.gather(tm['wgz'], G) + tm['wbz'] + tf.reshape(tf.matmul(X, tm['wz']), [-1])\n",
    "        #Pz_link = tm['wbz'] + tf.reshape(tf.matmul(X, tm['wz']), [-1])\n",
    "        #tf.summary.histogram('Pz_link', Pz_link)\n",
    "        Pz = self.zero_link_inverse(Pz_link, Ps)\n",
    "        \n",
    "        # Positive count expectation\n",
    "        Yv_mu_link = tf.gather(tm['wgv'], Gv) + tm['wbv'] + tf.reshape(tf.matmul(Xv, tm['wv']), [-1])\n",
    "        #Yv_mu_link = tm['wbv'] + tf.reshape(tf.matmul(Xv, tm['wv']), [-1])\n",
    "        Yv_mu = self.value_link_inverse(Yv_mu_link)\n",
    "        \n",
    "        Yv_mu_samp = self.value_link_inverse(\n",
    "            tf.gather(tm['wgv'], G) + tm['wbv'] + tf.reshape(tf.matmul(X, tm['wv']), [-1])\n",
    "        )\n",
    "        #Yv_mu = tf_print(Yv_mu, lambda x: pd.Series(x).quantile(q=[0, .1, .9, 1]).values)\n",
    "        \n",
    "        def add_yvmu(w):\n",
    "            global YVMU\n",
    "            YVMU.append(w)\n",
    "            return None\n",
    "        #Yv_mu = tf_printv(Yv_mu, add_yvmu)\n",
    "        \n",
    "        #Yv_mu = tf_print(Yv_mu, lambda x: [x.min(), x.max()])\n",
    "        #tf.summary.histogram('Yv_mu', tf.clip_by_value(Yv_mu, -10000., 10000.))\n",
    "        #tf.summary.scalar('Yv_mu', tf.reduce_mean(Yv_mu))\n",
    "        #tf.summary.histogram('Yv_mu_link', Yv_mu_link)\n",
    "        \n",
    "        #tf.summary.histogram('Yv_mu_link_sat', Yv_mu_link_sat)\n",
    "        \n",
    "        \n",
    "        # ##### Error Family Mapping #### #\n",
    "        cYz = Bernoulli(probs=Pz)\n",
    "        \n",
    "        # Positive value error distribution\n",
    "        if self.link == 'nbinom':\n",
    "            tm['Ys'] = tf.cast(self.ys_max * tf.exp(tf.Variable(np.log(1./self.ys_max))), tf.float32)\n",
    "            #tm['Ys'] = .2\n",
    "            tf.summary.scalar('Ys', tm['Ys'])\n",
    "            cYv = NegativeBinomial(tm['Ys'] * tf.ones_like(Yv_mu), probs=Yv_mu / (Yv_mu + tm['Ys']))\n",
    "            cYv_samp = NegativeBinomial(tm['Ys'] * tf.ones_like(Yv_mu_samp), probs=Yv_mu_samp / (Yv_mu_samp + tm['Ys']))\n",
    "        else:\n",
    "            cYv = Poisson(Yv_mu)\n",
    "            cYv_samp = Poisson(Yv_mu_samp)\n",
    "            #cYv = Normal(Yv_mu, 1. * tf.ones_like(Yv_mu))\n",
    "            #cYv_samp = Normal(Yv_mu_samp, 1. * tf.ones_like(Yv_mu_samp))\n",
    "        \n",
    "        # Sampling\n",
    "        tm['n_samp'] = tf.placeholder(tf.int32)\n",
    "        \n",
    "        tm['Yz_samp'] = ed.copy(cYz, lm).sample(tm['n_samp'])\n",
    "        #tm['Yz_pred'] = ed.copy(Pz_samp, lm)\n",
    "        #tm['Yz_prob'] = ed.copy(Yz, lm).pmf(dIz.astype(np.int32))\n",
    "        \n",
    "        tm['Yv_samp'] = ed.copy(cYv_samp, lm).sample(tm['n_samp'])\n",
    "        #tm['Yv_pred'] = ed.copy(Yv_link, lm)\n",
    "        #tm['Yv_prob'] = ed.copy(cYv, lm).log_prob(dY.astype(np.float32))\n",
    "\n",
    "        \n",
    "        def input_fn(d):\n",
    "            return {\n",
    "                Xp: dX, Yp: dY, G: dG,\n",
    "                cYz: (dY == 0).astype(np.int32),\n",
    "                cYv: (dY[dY > 0] - 1).astype(np.int32)\n",
    "            }\n",
    "        \n",
    "        return input_fn, lm, tm\n",
    "        \n",
    "    def criticism_args(self, sess, tm):\n",
    "        \n",
    "        def sample_fn(n, X, G):\n",
    "            G = self.group_encoder_.transform(G).astype(np.int32)\n",
    "            Yz, Yv = sess.run(\n",
    "                [tm['Yz_samp'], tm['Yv_samp']], \n",
    "                feed_dict={tm['Xp']: X, tm['G']: G, tm['n_samp']: n}\n",
    "            )\n",
    "            assert Yz.shape == (n, X.shape[0])\n",
    "            assert Yv.shape == (n, X.shape[0])\n",
    "            \n",
    "            Yv = Yv + 1 # Unshift\n",
    "            Y = np.multiply(1. - Yz, Yv)\n",
    "            if self.y_thresh is not None:\n",
    "                Y = np.clip(Y, -np.inf, self.y_thresh)\n",
    "            return Y\n",
    "        \n",
    "#         def mean_pred_fn(X, G):\n",
    "#             G = self.group_encoder_.transform(G).astype(np.int32)\n",
    "#             Yv = sess.run(\n",
    "#                 tm['Yv_pred'], \n",
    "#                 feed_dict={tm['Xp']: X, tm['G']: G}\n",
    "#             )\n",
    "#             assert Yv.ndim == 1 and Yv.shape[0] == X.shape[0]\n",
    "            \n",
    "#             #print(pd.Series(self.y_scaler_.inverse_transform(Yv)).describe())\n",
    "#             #Yv = np.exp(self.y_scaler_.inverse_transform(Yv) + .5 * (Ys**2)) # Arithmetic mean of log-normal\n",
    "\n",
    "#             return Yv\n",
    "        \n",
    "        \n",
    "#         def loglik_fn(X, G, train_data):\n",
    "#             G = self.group_encoder_.transform(G).astype(np.int32)\n",
    "#             Yv = sess.run(\n",
    "#                 tm['Yv_prob'], \n",
    "#                 feed_dict={tm['Xp']: X, tm['G']: G}\n",
    "#             )\n",
    "            \n",
    "#             X, y = train_data\n",
    "#             idx_pos = np.argwhere(~tm['dIz'])[:,0]\n",
    "#             assert y.iloc[idx_pos].min() > 0\n",
    "            \n",
    "#             d_prob = pd.DataFrame({\n",
    "#                 'y_pos': y.iloc[idx_pos].values,\n",
    "#                 'y_pos_prep': m.tensor_map_['dYv'],\n",
    "#                 'y_pos_raw': m.tensor_map_['dYv_raw'][:, 0],\n",
    "#                 'y_prob': Yv\n",
    "#             }, index=y.iloc[idx_pos].index)\n",
    "#             assert np.all(d_prob['y_pos'] == d_prob['y_pos_raw'])\n",
    "\n",
    "#             d_prob = pd.concat([\n",
    "#                 X, y.rename('ll:y_actual'), d_prob.add_prefix('ll:'),\n",
    "#                 pd.Series(Yz, index=y.index, name='ll:zero_prob')\n",
    "#             ], axis=1)\n",
    "            \n",
    "#             d_prob['ll:pos_prob'] = 1. - d_prob['ll:zero_prob']\n",
    "            \n",
    "#             return d_prob\n",
    "                 \n",
    "        return {\n",
    "            'sample_fn': sample_fn, \n",
    "#             'mean_pred_fn': mean_pred_fn,\n",
    "#             'loglik_fn': loglik_fn\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Invocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#inference_fn=ed.MAP\n",
    "inference_fn=ed.KLqp\n",
    "\n",
    "def get_model():\n",
    "    y_max = max(y.max(), y_test.max())\n",
    "    return HurdleCountModel(inference_fn, y_thresh=y_max, p_sat_max=.1, link='nbinom')\n",
    "\n",
    "model = get_model()\n",
    "est = ed_models.BayesianModelEstimator(\n",
    "    model, n_collect=1, n_print_progress=30, max_steps=1500,\n",
    "    random_state=1, fail_if_not_converged=False,\n",
    "    inference_fn=inference_fn, n_samples=6, \n",
    "    tol=.05 if inference_fn == ed.MAP else 1.,\n",
    "    #optimizer='adagrad'\n",
    ")\n",
    "! rm /tmp/sim/zicount/*\n",
    "est.set_log_dir('/tmp/sim/zicount')\n",
    "est = Pipeline([\n",
    "    #('scale', StandardScaler()),\n",
    "    ('est', est)\n",
    "])\n",
    "\n",
    "\n",
    "#est.fit(X.head(10000).values, y.head(10000).values, est__groups=X.head(10000).index.get_level_values('employer'))\n",
    "#est.fit(X.values, y.values, est__groups=X.index.get_level_values('employer'))\n",
    "est.fit(X.values, y.values, est__groups=X.index.get_level_values('employer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Inflated Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Normal, Gamma, Bernoulli, PointMass, Uniform, Laplace, InverseGamma\n",
    "from ml.tensorflow.utilities import tf_print\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "\n",
    "class ZICostModel(ed_models.BayesianModel):\n",
    "    \n",
    "    def __init__(self, x_thresh=10., z_thresh=25., family='normal', link_fn=tf.identity, y_scaling=False):\n",
    "        self.sample_fn_ = None\n",
    "        self.x_thresh = x_thresh\n",
    "        self.z_thresh = z_thresh\n",
    "        assert family in ['normal', 'gamma']\n",
    "        self.family = family\n",
    "        self.link_fn = link_fn\n",
    "        self.group_encoder_ = None\n",
    "        self.y_scaling = y_scaling\n",
    "        self.y_scaler_ = None\n",
    "        \n",
    "    def inference_args(self, data, groups=None):\n",
    "        \n",
    "        tm, lm = {}, {}\n",
    "        \n",
    "        # Extract true X and Y values, clipping X values to be\n",
    "        # <= self.x_thresh standard deviations (crucial for gradient descent to not give nans)\n",
    "        dX, dY = np.clip(data['X'], -self.x_thresh, self.x_thresh), data['Y']\n",
    "        tm['dX'] = dX\n",
    "        dzi = dY < ph_modeling.COST_ZERO_THRESH\n",
    "        #print(pd.Series(dzi).value_counts())\n",
    "        dXv, dYv = dX[~dzi, :], dY[~dzi]\n",
    "        if self.y_scaling:\n",
    "            #self.y_scaler_ = MinMaxScaler((1., 100.))\n",
    "            self.y_scaler_ = FunctionTransformer(func=lambda x: x / 10000., inverse_func=lambda x: x * 10000.)\n",
    "            #print(dYv.shape)\n",
    "            dYv = self.y_scaler_.transform(np.reshape(dYv, [-1, 1]))[:, 0]\n",
    "            #print(dYv.shape)\n",
    "            \n",
    "        dXz = dX[dzi, :]\n",
    "        \n",
    "        #print(pd.Series(dYv).describe())\n",
    "        \n",
    "        assert groups is not None\n",
    "        assert len(groups) == dX.shape[0]\n",
    "        nG = len(np.unique(groups))\n",
    "        self.group_encoder_ = LabelEncoder().fit(groups)\n",
    "        dG = self.group_encoder_.transform(groups).astype(np.int32)\n",
    "        dGv = self.group_encoder_.transform(groups).astype(np.int32)[~dzi]\n",
    "        \n",
    "        # print(pd.Series(dYz).describe(), pd.Series(dYv).describe())\n",
    "        \n",
    "        P = dX.shape[1]\n",
    "        X = tf.constant(dX, dtype=tf.float32)\n",
    "        Xv = tf.constant(dXv, dtype=tf.float32)\n",
    "        \n",
    "        # ##### Weights #### #\n",
    "\n",
    "        def get_ranef(mu, sigma):\n",
    "            return Normal(mu, sigma)\n",
    "        \n",
    "        def get_fixef(mu, sigma):\n",
    "            return Laplace(mu, sigma)\n",
    "        \n",
    "        # Group coefficients\n",
    "        Gz = get_ranef(tf.zeros([nG]), 10.*tf.ones([nG]))\n",
    "        qGz = PointMass(params=tf.Variable(tf.random_normal([nG], stddev=.1)))\n",
    "        #qGz = PointMass(params=tf.zeros([nG]))\n",
    "        bGz = tf.gather(Gz, dG)\n",
    "        lm[Gz], tm['qGz'] = qGz, qGz.params\n",
    "        tf.summary.histogram('qGz', qGz.params)\n",
    "        \n",
    "        Gv = get_ranef(tf.zeros([nG]), 10.*tf.ones([nG]))\n",
    "        qGv = PointMass(params=tf.Variable(tf.random_normal([nG], stddev=.1)))\n",
    "        #qGv = PointMass(params=tf.zeros([nG]))\n",
    "        bGv = tf.gather(Gv, dGv)\n",
    "        lm[Gv], tm['qGv'] = qGv, qGv.params\n",
    "        tf.summary.histogram('qGv', qGv.params)\n",
    "        \n",
    "        # Zero-value coefficients\n",
    "        Bz = get_fixef(tf.zeros([P, 1]), 1.*tf.ones([P, 1]))\n",
    "        qBz = PointMass(params=tf.Variable(tf.random_normal([P, 1], stddev=.1)))\n",
    "        lm[Bz], tm['qBz'] = qBz, qBz.params\n",
    "        \n",
    "        Bz0 = get_fixef(tf.zeros([]), 10.)\n",
    "        qBz0 = PointMass(params=tf.Variable(tf.random_normal([], stddev=.1)))\n",
    "        lm[Bz0], tm['qBz0'] = qBz0, qBz0.params\n",
    "        tf.summary.histogram('qBz0', qBz0.params)\n",
    "        \n",
    "        \n",
    "        # Positive-value coefficients\n",
    "        Bv = get_fixef(tf.zeros([P, 1]), 1.*tf.ones([P, 1]))\n",
    "        qBv = PointMass(params=tf.Variable(tf.random_normal([P, 1], stddev=.1)))\n",
    "        lm[Bv], tm['qBv'] = qBv, qBv.params\n",
    "        \n",
    "        Bv0 = get_fixef(tf.zeros([]), 10.)\n",
    "        qBv0 = PointMass(params=tf.Variable(tf.random_normal([], stddev=.1)))\n",
    "        lm[Bv0], tm['qBv0'] = qBv0, qBv0.params\n",
    "        tf.summary.histogram('qBv0', qBv0.params)\n",
    "\n",
    "        \n",
    "        # ##### Expectations #### #\n",
    "        \n",
    "        # Compute probability of a zero value\n",
    "        Pz_link = bGz + Bz0 + tf.reshape(tf.matmul(X, Bz), [-1])\n",
    "        Pz_link = tf.clip_by_value(Pz_link, -self.z_thresh, self.z_thresh)\n",
    "        \n",
    "        # Compute expectation of positive values\n",
    "        Yv_link = bGv + Bv0 + tf.reshape(tf.matmul(Xv, Bv), [-1])\n",
    "        Yv_mid = self.link_fn(Yv_link)\n",
    "        #Yv_mid = tf_print(Yv_mid, lambda x: [x.min(), x.max()])\n",
    "\n",
    "        \n",
    "        # ##### Error Family Mapping #### #\n",
    "        \n",
    "        # Map positivity predictions\n",
    "        Yz = Bernoulli(p=tf.nn.sigmoid(Pz_link))\n",
    "        \n",
    "        # Map non-zero value predictions\n",
    "        if self.family == 'gamma':\n",
    "            # Create Gamma scale parameter\n",
    "            # See here for the rationale behind this parameterization:\n",
    "            # http://seananderson.ca/2014/04/08/gamma-glms.html\n",
    "            Ys = Uniform(.001, 100.)\n",
    "            #Ys = Normal(5., 1.)\n",
    "            #qYs = PointMass(params=tf.clip_by_value(tf.nn.softplus(tf.Variable(10.)), .001, 100.))\n",
    "            qYs = PointMass(params=tf.exp(tf.nn.softplus(tf.Variable(1.))))\n",
    "            #qYs = 5.\n",
    "            # Yv = Gamma(alpha=Ys * tf.ones_like(Yv_mid), beta=Ys / Yv_mid)\n",
    "            Yv = Gamma(alpha=Ys * tf.ones_like(Yv_mid), beta=Ys / Yv_mid)\n",
    "        elif self.family == 'normal':\n",
    "            # Create Normal variance parameter\n",
    "            #Ys = InverseGamma(.001, .001)\n",
    "            Ys = Uniform(0., 100000.)\n",
    "            qYs = PointMass(params=tf.nn.softplus(tf.Variable(1.)))\n",
    "            Yv = Normal(mu=Yv_mid, sigma=tf.sqrt(Ys * tf.ones_like(Yv_mid)))\n",
    "            \n",
    "        lm[Ys], tm['qYs'] = qYs, qYs.params\n",
    "        #tm['qYs'] = qYs\n",
    "        tf.summary.scalar('qYs', tm['qYs'])\n",
    "        \n",
    "        def add_sampling_graph():\n",
    "            X = tf.placeholder(tf.float32, shape=[None, P])\n",
    "            G = tf.placeholder(tf.int32, shape=[None])\n",
    "            X_clip = tf.clip_by_value(X, -self.x_thresh, self.x_thresh)\n",
    "            n = tf.placeholder(tf.int32, shape=[])\n",
    "            \n",
    "            Pz_link = tf.gather(tm['qGz'], G) + tm['qBz0'] + tf.reshape(tf.matmul(X_clip, tm['qBz']), [-1])\n",
    "            Pz_link = tf.clip_by_value(Pz_link, -self.z_thresh, self.z_thresh)\n",
    "            \n",
    "            Yv_link = tf.gather(tm['qGv'], G) + tm['qBv0'] + tf.reshape(tf.matmul(X_clip, tm['qBv']), [-1])\n",
    "            Yv_mid = self.link_fn(Yv_link)\n",
    "            \n",
    "            Ynz = Bernoulli(p=1. - tf.nn.sigmoid(Pz_link)).sample(n)\n",
    "            \n",
    "            if self.family == 'gamma':\n",
    "                Yv = Gamma(alpha=tm['qYs']*tf.ones_like(Yv_mid), beta=tm['qYs']/Yv_mid).sample(n)\n",
    "            elif self.family == 'normal':\n",
    "                Yv = Normal(mu=Yv_mid, sigma=tf.sqrt(tm['qYs'] * tf.ones_like(Yv_mid))).sample(n)\n",
    "                \n",
    "            Y = tf.clip_by_value(tf.multiply(tf.cast(Ynz, tf.float32), Yv), 0., np.inf)\n",
    "            return n, X, G, Y\n",
    "        \n",
    "        def add_prediction_graph():\n",
    "            X = tf.placeholder(tf.float32, shape=[None, P])\n",
    "            G = tf.placeholder(tf.int32, shape=[None])\n",
    "            X_clip = tf.clip_by_value(X, -self.x_thresh, self.x_thresh)\n",
    "            \n",
    "            Pz_link = tf.gather(tm['qGz'], G) + tm['qBz0'] + tf.reshape(tf.matmul(X_clip, tm['qBz']), [-1])\n",
    "            Pz = tf.nn.sigmoid(tf.clip_by_value(Pz_link, -self.z_thresh, self.z_thresh))\n",
    "            \n",
    "            Yv_link = tf.gather(tm['qGv'], G) + tm['qBv0'] + tf.reshape(tf.matmul(X_clip, tm['qBv']), [-1])\n",
    "            Yv_mid = self.link_fn(Yv_link)\n",
    "            \n",
    "            Y = tf.clip_by_value(tf.multiply(Yv_mid, (1.-Pz)), 0., np.inf)\n",
    "            return X, G, Y\n",
    "        \n",
    "        \n",
    "        \n",
    "        tm['n_samp'], tm['X_samp'], tm['G_samp'], tm['Y_samp'] = add_sampling_graph()\n",
    "        tm['X_pred'], tm['G_pred'], tm['Y_pred'] = add_prediction_graph()\n",
    "            \n",
    "        def input_fn(d):\n",
    "            return {\n",
    "                Yz: dzi.astype(np.int64),\n",
    "                Yv: dYv\n",
    "            }\n",
    "        \n",
    "        return input_fn, lm, tm\n",
    "        \n",
    "    def criticism_args(self, sess, tm):\n",
    "        \n",
    "        def sample_fn(n, X, G):\n",
    "            G = self.group_encoder_.transform(G).astype(np.int32)\n",
    "            y = sess.run(tm['Y_samp'], feed_dict={tm['X_samp']: X, tm['G_samp']: G, tm['n_samp']: n})\n",
    "            if self.y_scaling:\n",
    "                y = self.y_scaler_.inverse_transform(y)\n",
    "            return y\n",
    "        \n",
    "        def mean_pred_fn(X, G):\n",
    "            G = self.group_encoder_.transform(G).astype(np.int32)\n",
    "            y = sess.run(tm['Y_pred'], feed_dict={tm['X_pred']: X, tm['G_pred']: G}) \n",
    "            if self.y_scaling:\n",
    "                y = self.y_scaler_.inverse_transform(y)\n",
    "            return y\n",
    "                        \n",
    "        return {\n",
    "            'sample_fn': sample_fn, \n",
    "            'mean_pred_fn': mean_pred_fn,\n",
    "            'pred_fn': mean_pred_fn\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Log-scale models\n",
    "model = ZICostModel(x_thresh=10., z_thresh=10., link_fn=tf.identity, family='normal')\n",
    "\n",
    "# Original-scale models\n",
    "#model = ZICostModel(x_thresh=10., z_thresh=10., link_fn=tf.nn.softplus, family='normal', y_scaling=True)\n",
    "#model = ZICostModel(x_thresh=10., z_thresh=10., link_fn=tf.nn.softplus, family='gamma', y_scaling=True)\n",
    "\n",
    "est = ed_models.BayesianModelEstimator(\n",
    "    model, n_collect=1, n_print_progress=30, max_steps=1500,\n",
    "    random_state=1, fail_if_not_converged=False,\n",
    "    inference_fn=ed.MAP\n",
    ")\n",
    "! rm /tmp/sim/zicost/*\n",
    "est.set_log_dir('/tmp/sim/zicost')\n",
    "est = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('est', est)\n",
    "])\n",
    "est.fit(X.values, y.values, est__groups=X.index.get_level_values('employer'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
