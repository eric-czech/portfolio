---
title: "R Notebook for Alzheimers Analysis"
author: Eric Czech
output: html_notebook
---

This doc will cover some applications of simple and complex models to our Alzheimer's dataset, and talk about how the results could be applied or interpereted much like the work originally done in the paper ["Multiplexed immunoassay panel identifies novel CSF biomarkers for Alzheimer's disease diagnosis and prognosis"](https://www.ncbi.nlm.nih.gov/pubmed/21526197]).

Some of the topics covered:

- How to make custom Caret models (and why this is usually the best route to go for anything non-standard)
- Building ensemble models with [caretEnsemble](https://cran.r-project.org/web/packages/caretEnsemble/vignettes/caretEnsemble-intro.html)
- A little bit on classifier performance metrics when [Facing Imbalanced Data](http://www.pitt.edu/~jeffcohn/biblio/Jeni_Metrics.pdf) (figure 1 on page 5 of this paper is the most informative thing I have ever seen on this topic)
- A little bit more on plot.ly and its awesomeness
- Understanding what black-box ML models learn with partial dependence, and where [PDP can fail](https://arxiv.org/pdf/1309.6392v2.pdf) (the figure on page 5 is a great example)
- Modeling philosophies w.r.t. linear vs non-linear models
- How to find a middle ground between dumb, linear statistical models that generalize (i.e. make predictions) poorly and ML algos that generalize well but make you feel dumb


```{r init, results='hide', warning=F, message=F, error=F, echo=F}
source('common.R')
library(ggplot2)
library(corrplot)
library(stringr)
library(reshape2)
library(plotly)
library(knitr)
layout <- plotly::layout

# Paper on data:
# Multiplexed Immunoassay Panel Identifies Novel CSF Biomarkers for Alzheimer's Disease Diagnosis and Prognosis
# library(AppliedPredictiveModeling)
# ?diagnosis
```

# Refresher on Our AD Data

Load in the same data as before and run some basic analysis to get back in the munging mood:

```{r}
# Load in the whole Alzheimers data file
data <- read.csv('~/repos/portfolio/demonstrative/R/datasets/alzheimers/alzheimers.csv')

# Remove this field ... I forgot to do that when creating the dataset
data <- data %>% select(-male)

# Normalize gender labels
stopifnot(all(!is.na(data$gender)))
normalize.gender <- function(x) {
  is.male <- x %>% tolower %>% str_detect('^m')
  ifelse(is.male, 'Male', 'Female') %>% factor
}
data <- data %>% mutate(gender=normalize.gender(gender))

# Convert integer fields to numeric for the sake of consistency
data <- data %>% mutate_each_(funs(as.numeric), c('Betacellulin', 'Eotaxin_3'))

head(data)
```


```{r}
# Parse our data into predictors and a binary response 
# (Impaired vs Not Impaired) and check response frequency
X <- data %>% select(-response)
y <- data[,'response']
table(y)
```

```{r}
table(y) / length(y)
```

```{r}
# Check out the names of the predictors we have:
names(X)
```

```{r}
# Base R to accomplish the same:
# table(sapply(X, class))

# Look at what class each predictor has:
X %>% sapply(class) %>% table
```

```{r}
# Pipeline to accomplish the same:
# X %>% sapply(class) %>% .[. == 'factor']

# Identify the factor variables, since they also need special attention:
names(X)[sapply(X, class) == 'factor']
```

# Partial Analysis

We can start by looking at the relationship between some of the more intuitive variables like Age, Gender, and Genotype and impairment, to see if there are any obvious relationships.

## Age vs Impairment

```{r}
data %>% 
  mutate(age_range=cut(age, breaks=5)) %>%
  group_by(age_range, response) %>% tally %>% 
  plot_ly(x=~age_range, y=~n, color=~response, type='bar') %>%
  plotly::layout(hovermode='closest', title='Age vs Impairment')
```


## Genotype vs Impairment

```{r}
data %>% 
  group_by(Genotype, response) %>% tally %>% 
  plot_ly(x=~Genotype, y=~n, color=~response, type='bar') %>%
  plotly::layout(hovermode='closest', title='Genotype vs Impairment')
```


## Gender vs Impairment

```{r}
data %>% 
  group_by(gender, response) %>% tally %>% 
  plot_ly(x=~gender, y=~n, color=~response, type='bar') %>%
  layout(hovermode='closest', title='Gender vs Impairment')
```


### Protein Analysis

Now what about the other ~125 variables?  We can't look at them all one at a time so perhaps there is a way to "condense" them into something more manageable:


```{r, fig.width=10, fig.height=10}
library(corrplot)
d.pca <- X %>% select(-gender, -Genotype)
cor.mat <- corrplot(cor(d.pca), order='hclust', tl.col='black', tl.cex=.5)

# Extract the variable names from the figure below since they will be useful
# for creating similar plots for comparison (and it's easier to compare in the same order)
cor.var <- rownames(cor.mat)
```

## PCA 

```{r}
# Run PCA, and make sure scaling is on since we didn't do that manually
pca <- prcomp(d.pca, scale=T)
```


Check how well the Principal Components capture the correlated groups of variables:

```{r}
data.frame(Cumulative.Variance=summary(pca)$importance[3,]) %>% mutate(PC.Number=1:n()) %>%
  plot_ly(x=~PC.Number, y=~Cumulative.Variance, mode='lines', type='scatter') %>%
  layout(title='Cumulative Variance Explained by Principal Components')
```


## Projections of Features onto PCs

One way to look at how much each PC is related to a feature is to look at the correlation between the feature itself and it's transformed version in the new Principal Compenent space .. err a simpler way to put that is to say we have some "transformation" (which could be from PCA or anything else), which will assign a new value to every feature for each observation.  Then we can just look at how the original values correlate with the transformed ones to see what the transformation is actually doing:

```{r, fig.width=4, fig.height=10}
original.data <- d.pca[,cor.var]
pca.data <- pca$x[,1:25] # Using the first 25 PCs only, for brevity
corrplot(cor(original.data, pca.data), tl.col='black', tl.cex=.5)
```


# Applying and Analyzing Predictive Models

## Modeling Data Prep

First we have to do a little better of extra massaging of the predictor data to make sure that all the different model types within caret will be able to use it (namely, encoding factors in some numeric way).

```{r}
# We currently have factors like this:
X %>% select(starts_with('gender'), starts_with('Genotype')) %>% head
```

We don't want these to remain factors or a lot of different caret models will choke.  Luckily there's a caret function that makes that pretty easy:

```{r}
# Here, the dummyVars function will automatically detect and turn factor values into separate columns
X.m <- predict(dummyVars(~., X), X) %>% as.data.frame
X.m %>% select(starts_with('gender'), starts_with('Genotype')) %>% head
```


### Model Helper Functions

These can be ignored for now but are good to come back to for reference on specific configurations and such:

```{r}
# These functions are mostly just precursors to the following training commands, and
# serve to do things like specify cross validation, how performance measures are 
# calculated, create ensemble models, apply PCA to data subsets, etc.
#
# This isn't a caret thing, it's just my own thing I use to help 
# make it more convenient to save model results on disk in a more 
# convenient way
tr <- proj$getTrainer()
#
# This function with define how performance measures are calculated for each "fold"
summary.function <- function(...){
  arg.list <- list(...)
  if (!('Impaired' %in% names(arg.list[[1]])))
    arg.list[[1]]$Impaired <- arg.list[[1]]$classProb
  c(do.call(twoClassSummary, arg.list), do.call(defaultSummary, arg.list))
}
#
# Training control -- standard caret stuff
tc <- trainControl(
  classProbs=T, method='cv', number=10,
  summaryFunction = summary.function,
  verboseIter=F, savePredictions='final', returnResamp='final', allowParallel=T
)
#
# This is an ensemble specification that will average predictions from 
# multiple models together
get.ensemble.model <- function(tuneList){
  caret.list.args <- list(
    trControl=trainControl(
      method='cv', number=5, classProbs=T,
      summaryFunction = function(...) c(twoClassSummary(...), defaultSummary(...)),
      returnData=F, savePredictions='final',
      allowParallel=T, verboseIter=F
    ),
    tuneList=tuneList
  )
  caret.stack.args <- list(
    method=GetEnsembleAveragingModel(),
    trControl=trainControl(method='none', savePredictions = 'final', classProbs=T,
                           summaryFunction = function(...) c(twoClassSummary(...), defaultSummary(...)),
                           returnResamp='final')
  )
  GetCaretEnsembleModel(caret.list.args, caret.stack.args)
}
#
# Create a vector containing the names of features we do NOT want to preprocess with PCA
non.pca.vars <- X.m %>% select(starts_with('Gender'), starts_with('Genotype'), age) %>% names
#
# Two utility functions for splitting predictor data into separate data frames as well
# as merging them back together before fitting models with the result
pca.split <- function(X) {
  list(
    var=X %>% dplyr::select(one_of(non.pca.vars)),
    pca=X %>% dplyr::select(-one_of(non.pca.vars))
  )
}
pca.combine <- function(pp, X) {
  X <- pca.split(X)
  cbind(X$var, predict(pp, newdata=X$pca))
}
```

# Custom Caret Model Spec

In order to use PCA within the caret modeling framework, you have 3 main options:

1. Using it's built in support for preprocessing data with PCA, which is great, but does not at all support only applying PCA to some of the data
2. Ignore caret, write your own CV loops and results aggregation stuff
3. Wrap caret model implementations with some functions that do the application of PCA for you

Option 3 is likely the best choice and caret makes it easy to override any part of its built in models (though understanding the consequences of overriding things is sometimes tough).  Here is an example of this below:

```{r}
# Example Caret custom model specification

# PCA preprocessing model wrapper 
# 
# This function will take any valid caret "method" name
# and wrap its fit/predict/prob functions with necessary logic to
# apply PCA to data subsets
get.model <- function(model.name){
  # All Caret models are represented as lists, retrievable using "getModelInfo"
  # Each of these lists has a variety of functions keyed by names like:
  # "fit" - This function takes in data to fit the model on and should return a fit model object
  # "prob" - This function in the list should return predicted probabilities
  # "grid" - This function is responsible for creating hyperparameter grids to train over
  # And several more ...
  model <- getModelInfo()[[model.name]]
  m <- model
  
  # In this case, we're overriding the standard "fit" function with a new function
  # that will split the input data, apply PCA to some of it, and then fit a model
  # on the resulting, smaller dataset
  m$fit <- function(x, y, wts, param, lev, last, classProbs, ...){
    X <- pca.split(x)
    pp <- preProcess(X$pca, method=c('center', 'scale', 'pca'), pcaComp = 40)
    X.train <- cbind(X$var, predict(pp, newdata=X$pca))
    modelFit <- model$fit(X.train, y, wts, param, lev, last, classProbs, ...)
    modelFit$pp <- pp
    modelFit$feature.names <- names(X.train)
    modelFit
  }
  
  # The "predict" and "prob" methods must be overriden as well to apply PCA
  # to any new data given to make predictions on
  m$predict <- function (modelFit, newdata, submodels = NULL) {
    X.test <- pca.combine(modelFit$pp, newdata)
    model$predict(modelFit, X.test, submodels)
  }
  m$prob <- function (modelFit, newdata, submodels = NULL){
    X.test <- pca.combine(modelFit$pp, newdata)
    model$prob(modelFit, X.test, submodels)
  }
  
  # And as is par for the course with Caret, there are always a couple
  # of model types that don't always work as expected when you do custom
  # things like this.  In this case, xgbTree models were using the original
  # feature names to get "importances" of each so that must be overriden
  # here to use the reduced feature list instead
  if (model.name == 'xgbTree'){
    m$varImp = function(object, numTrees = NULL, ...) {
      imp <- xgb.importance(object$feature.names, model = object)
      imp <- as.data.frame(imp)[, 1:2]
      rownames(imp) <- as.character(imp[,1])
      imp <- imp[,2,drop = FALSE]
      colnames(imp) <- "Overall"
      imp   
    }
  }
  m
}
```

## Model Training

With all the precursor stuff in place, the caret models can be called with different tuning settings and in this case, the wrapper function above will also make sure PCA is applied to protein features:

```{r}
models <- list(
  tr$getModel('pca_glm', method=get.model('glm'), trControl=tc),
  tr$getModel('pca_glmnet', method=get.model('glmnet'), trControl=tc, tuneLength=5),
  tr$getModel('pca_rf', method=get.model('rf'), trControl=tc, tuneGrid=expand.grid(.mtry = c(2,4,8))),
  tr$getModel('pca_rpart', method=get.model('rpart'), tuneLength=10, trControl=tc),
  tr$getModel('pca_gbm', method=get.model('gbm'), tuneLength=5, trControl=tc, verbose=F),
  tr$getModel('pca_xgb', method=get.model('xgbTree'), tuneLength=5, trControl=tc),
  tr$getModel('pca_spline', method=get.model('earth'), preProcess=pre.proc, trControl=tc, tuneLength=5),
  tr$getModel('pca_nnet', method=get.model('nnet'), preProcess=pre.proc, trControl=tc, tuneLength=5, trace=F)
)
names(models) <- sapply(models, function(m) m$name)

# Loop through the models and call "train" on each
pca.results <- lapply(models, function(m) tr$train(m, X.m, y, enable.cache=T)) %>% setNames(names(models))
```

## Non-PCA Model Training

For comparison, also train a bunch of models on the entire dataset:

```{r}
library(caretEnsemble)
pre.proc <- c('center', 'scale')

# This "ensemble" model will combine predictions from 3 separate other models
ens.model <- list(
  glmnet=caretModelSpec(method='glmnet', preProcess=pre.proc, tuneLength=5),
  gbm=caretModelSpec(method='gbm', tuneLength=5, verbose=F),
  spline=caretModelSpec(method='earth', tuneLength=5, preProcess=pre.proc)
)
models <- list(
  tr$getModel('glm', method='glm', preProcess=pre.proc, trControl=tc),
  tr$getModel('glmnet', method='glmnet', preProcess=pre.proc, trControl=tc, tuneLength=5),
  tr$getModel('nnet', method='nnet', preProcess=pre.proc, trControl=tc, tuneLength=5, trace=F),
  tr$getModel('rpart', method='rpart', tuneLength=10, trControl=tc),
  tr$getModel('gbm', method='gbm', tuneLength=5, trControl=tc, verbose=F),
  tr$getModel('xgb', method='xgbTree', tuneLength=5, trControl=tc),
  tr$getModel('spline', method='earth', preProcess=pre.proc, trControl=tc, tuneLength=5),
  tr$getModel('rf', method='rf', trControl=tc, tuneLength=5),
  tr$getModel('ensemble', method=get.ensemble.model(ens.model), trControl=tc)
)
names(models) <- sapply(models, function(m) m$name)

# Again, loop through the models and run the training process
all.results <- lapply(models, function(m) tr$train(m, X.m, y, enable.cache=T)) %>% setNames(names(models))
```

## Raw Results

Caret attaches a ton of information to each fit model object:

```{r}
# The "model" object itself
pca.results[['pca_glm']]$fit$finalModel
```


```{r}
# Resampling results for every hyperparameter combination
all.results[['xgb']]$fit$results %>% head
```


## Model Performance

```{r}
rbind(GetResampleData(pca.results), GetResampleData(all.results)) %>%
  mutate(model=reorder(model, kappa, median)) %>%
  plot_ly(x=~model, y=~kappa, type='box') %>%
  layout(margin=list(b=100), title='Model Performance')
```


```{r}
rbind(GetResampleData(pca.results), GetResampleData(all.results)) %>% head
```

```{r}
metric <- 'kappa'
dt <- rbind(GetResampleData(pca.results), GetResampleData(all.results)) 
dt.ens <- dt %>% filter(model == 'ensemble') %>% arrange(resample) %>% .[,metric]
dt %>% group_by(model) %>% do({
  d <- .
  d <- d %>% arrange(resample)
  # print(d)
  if (d$model[1] == 'ensemble'){
    data.frame(p=1)
  } else {
    r <- t.test(dt.ens, data.frame(d)[,metric], paired=T, alternative='greater')
    data.frame(p=r$p.value)
  }
}) %>% ungroup %>% arrange(p) %>% 
  mutate(model=reorder(model, p, max)) %>%
  plot_ly(x=~model, y=~p, type='scatter', mode='lines') %>%
  layout(margin=list(b=100,r=100), title='T-Test Statistics')
```

## Feature Importance

```{r}
var.imp <- GetVarImp(all.results)
var.imp %>%
  mutate(feature=reorder(var.imp$feature, var.imp$score, mean)) %>%
  plot_ly(x=~feature, y=~score, color=~model, mode='markers', type='scatter') %>%
  layout(margin=list(b=200), title='Feature Importance (no PCA Features)')
```



```{r}
var.model.names <- names(pca.results)[!str_detect(names(pca.results), 'nnet')]
var.imp <- GetVarImp(pca.results[var.model.names])
var.imp %>%
  mutate(feature=reorder(var.imp$feature, var.imp$score, mean)) %>%
  plot_ly(x=~feature, y=~score, color=~model, mode='markers', type='scatter') %>%
  layout(margin=list(b=200), title='Feature Importance (w/ PCA Features)')
```


<!-- {r} -->
<!-- #pc3 <- results$pca_xgb$fit$finalModel$pp$rotation[,3]  -->
<!-- scale_vec <- function(x) (x - median(x)) / IQR(x) -->
<!-- results$pca_xgb$fit$finalModel$pp$rotation %>% t %>% apply(2, scale_vec) %>%  -->
<!--   as.data.frame %>% add_rownames(var='PC') %>%  -->
<!--   plot_ly(x=~tau, y=~Ab_42, text=~PC, type='scatter', mode='markers') -->

## Partial Dependence Calculation

```{r}

pd.vars <- c(
  'age', 'gender.Male', 'gender.Female', 'tau', 
  'Cystatin_C', 'Ab_42', 'Cortisol', 'VEGF',
  'NT_proBNP'
)
pd.models <- c('xgb', 'gbm', 'glmnet', 'spline', 'ensemble')

pred.fun <- function(object, newdata) {
  if ('caretStack' %in% class(object$finalModel)){
    pred <- predict(object$finalModel, newdata=newdata, type='prob')
  } else {
    pred <- predict(object, newdata=newdata, type='prob')
  }
  if (is.vector(pred)) pred
  else pred[,1] 
}
options(error=recover)
registerCores(1) # Increase this to make PD calcs faster
pd.data <- GetPartialDependence(
  all.results[pd.models], pd.vars, pred.fun, 
  X=X.m, # This can come from model objects but only if returnData=T in trainControl
  grid.size=50, grid.window=c(0, 1), # Resize these to better fit range of data
  sample.rate=1, # Decrease this if PD calculations take too long
  verbose=F, seed=SEED
)

```


## Partial Dependence Plotting

First, to help understand where partial dependence comes from it's helpful to look at the "ICE" (Individual Conditional Expectation) curves.  This is a much simpler process than the name makes it sound .. computing these involves nothing more than picking an observation, altering the value of one predictor for that observation, and seeing how the predicted probability from a model changes as that predictor changes:

```{r, fig.width=8, fig.height=10}
pd.all <- foreach(pd=pd.data, .combine=rbind)%do%
{ pd$pd %>% dplyr::mutate(predictor=pd$predictor, model=pd$model) }

# Plot the ICE curves for every observation in our dataset (each line represents one observation)
pd.all %>% 
  mutate(i=as.numeric(i)) %>%
  ggplot(aes(x=x, y=y, group=i)) + 
  geom_line(show.legend=F, alpha=.1) + 
  theme_bw() +
  facet_wrap(predictor~model, scales='free', ncol = length(pd.models)) +
  ggtitle('Individual Conditional Expectation Curves (i.e. "Disaggregated" Partial Dependence)')
```

To make this simpler then, we could simply take the average predicted value across all the observations (i.e. individual lines) to give a single average predicted value for each feature value.  This is what partial dependence is:

```{r, fig.width=6, fig.height=3}
pd.hist.model <- pd.data[[1]]$model
pd.hist <- lapply(pd.data, function(pd){
  if (pd$model != pd.hist.model) return(NULL)
  else data.frame(pd$x) %>% setNames(pd$predictor)
}) %>% .[!sapply(., is.null)] %>% do.call('cbind', .) %>%
  melt(id.vars=NULL, variable.name='predictor')

pd.mean <- foreach(pd=pd.data, .combine=rbind)%do%
  { pd$pd %>% dplyr::mutate(predictor=pd$predictor, model=pd$model) } %>%
  dplyr::group_by(predictor, model, x) %>% 
  dplyr::summarise(y.mid=mean(y)) %>% ungroup 

ggplot(NULL) + 
  geom_rug(aes(x=value), size=2, alpha=.1, data=pd.hist) +
  geom_line(aes(x=x, y=y.mid, color=model), data=pd.mean) + 
  facet_wrap(~predictor, scale='free') +
  theme_bw() + 
  ylab('Predicted Probability') + xlab('Predictor Value') + 
  ggtitle('Partial Dependence by Model (free Y scale)')
```

The y-scales above vary widely, so fixing them to be the same gives a much clearer picture of what features are affecting predictions the most:

```{r, fig.width=6, fig.height=3}
ggplot(NULL) + 
  geom_rug(aes(x=value), size=2, alpha=.1, data=pd.hist) +
  geom_line(aes(x=x, y=y.mid, color=model), data=pd.mean) + 
  facet_wrap(~predictor, scale='free_x') +
  theme_bw() + 
  ylab('Predicted Probability') + xlab('Predictor Value') + 
  ggtitle('Partial Dependence by Model (fixed Y scale)')
```

## Model Coefficients

```{r}
glmnet.model <- all.results$glmnet$fit$finalModel
glmnet.coef <- predict(glmnet.model, s=all.results$glmnet$fit$bestTune$lambda, type='coefficients')
glmnet.coef <- glmnet.coef[,1]
glmnet.coef %>% data.frame %>% setNames('Coefficient') %>% add_rownames(var='Feature') %>%
  filter(abs(Coefficient) > 0) %>%
  mutate(Feature=reorder(Feature, Coefficient, mean)) %>%
  plot_ly(x=~Feature, y=~Coefficient, type='bar') %>%
  layout(margin=list(b=200), title='Glmnet Coefficients')
```

# Conclusion

What we've done here comes close to inferring similar conclusions as those in the paper that first used the same data (mentioned in the intro).  But, taking a lot of the same approaches into the world of business data leaves a lot to be desired IMO.  ML models certainly surface useful signals for making predictive conclusions, but the way they do so without any prior knowledge of the problem or the inputs really limits how realistic they can ever be (e.g. the step functions or unbounded relationships are never really true).

A good middle ground is likely frameworks for building models, rather than applying predefined ones (e.g. caret), and hopefully we make such things a topic for another meetup?  These kinds of frameworks make it possible to build models that generalize well but also learn realistic relationships, or at least in so far as they could ever be pre-determined.



<!-- Graveyard -->


<!-- # The code below will plot the PCA loadings (ie pca$rotation) matrix -->
<!-- # directly rather than looking at correlations between original and transformed variables -->
<!-- # (though these show about the same thing) -->

<!--
{r, fig.height=10, fig.width=6}
library(reshape2)
dp <- pca$rotation[,1:25][cor.var,] %>% as.data.frame %>%
  add_rownames(var='feature') %>%
  melt(id.vars='feature', variable.name='pc')
dp$feature <- factor(dp$feature, levels=rev(cor.var))
dp %>% ggplot(aes(x=pc, y=feature, fill=value)) + geom_tile() +
  scale_fill_gradient2(low='red', high='blue', mid='white')
-->

<!-- PCA + Data Projections -->
<!-- {r, fig.width=10, fig.height=10} -->
<!-- i.pca <- c(1,2) -->
<!-- d.pca.pred <- as.data.frame(predict(pca, d.pca)[,i.pca]) %>% setNames(., c('PC1', 'PC2')) -->
<!-- d.pca.pred$response <- y -->

<!-- # d.pca.pred %>% plot_ly(x=~PC1, y=~PC2, color=~response, type='scatter', mode='markers') -->

<!-- # Parameters for axis with no grid lines, ticks or labels -->
<!-- empty.axis <- list( -->
<!--   title = '', -->
<!--   zeroline = FALSE, -->
<!--   showline = FALSE, -->
<!--   showticklabels = FALSE, -->
<!--   ticklen = 0, -->
<!--   showgrid = FALSE -->
<!-- ) -->

<!-- # Create a line plot of each variable showing which direction it moves within our 2D space -->
<!-- p1 <- plot_ly( -->
<!--     d.pca.pred, x=~PC2, y=~PC1, type='scatter', color=~response,  -->
<!--     mode='markers', opacity=1 -->
<!--   ) %>%  -->
<!--   layout( -->
<!--     xaxis=list(showgrid=F, zeroline=T), -->
<!--     yaxis=list(showgrid=F, zeroline=T) -->
<!--   ) -->

<!-- # Create a heatmap of impairment incidence rate across our 2D space -->
<!-- d.hm <- d.pca.pred %>%  -->
<!--   mutate(PC1=as.character(cut(PC1, breaks=3)), PC2=as.character(cut(PC2, breaks=3))) %>% -->
<!--   group_by(PC1, PC2) %>% summarise(PCT=100*sum(response == 'Impaired')/n()) %>% -->
<!--   acast(PC1 ~ PC2, value.var='PCT') -->
<!-- p2 <- plot_ly(z=d.hm[c(3,2,1),], type='heatmap', reversescale=F) #%>% -->
<!--   #layout(xaxis=empty.axis, yaxis=empty.axis) -->

<!-- # Overlay the above plots on top of one another -->
<!-- subplot(p2, p1, margin=-1) %>%  -->
<!--   layout( -->
<!--     paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',  -->
<!--     width=750, height=500, -->
<!--     title='2D Projection of Correlated Features Overlayed w/ Impairment Rates' -->
<!--   ) -->



<!-- TSNE projections -->
<!-- {r} -->
<!-- library(tsne) -->
<!-- d.tsne <- X %>% select(-gender, -Genotype) -->
<!-- scale_vec <- function(x) (x - mean(x)) / sd(x) -->
<!-- d.tsne <- d.tsne %>% mutate_each(funs(scale_vec)) -->
<!-- m.tsne <- tsne(d.tsne) -->
<!-- m.tsne %>% as.data.frame %>% ggplot(aes(x=V1, y=V2)) + geom_point() -->

