result.handler, chunk.size)
}
}
}
library(dplyr)
library(XML)
library(foreach)
source('env.R')
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
names(d)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
names(d)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Player.rushing.Yds.Adj.None',
'Stat.Player.rushing.Avg.Ratio',
'Attr.Player.GF.1yr',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.RB.rushing.Avg.2nd'
)
d <- d[cols]
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.rushing.Yds',
'Stat.Player.rushing.Yds.Adj.None',
'Stat.Player.rushing.Avg.Ratio',
'Attr.Player.GF.1yr',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.RB.rushing.Avg.2nd'
)
d <- d[cols]
lm(Stat.Result.rushing.Yds ~ ., data=d)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.rushing.Yds',
'Stat.Player.rushing.Yds.Adj.None',
'Stat.Player.rushing.Avg.Ratio',
'Attr.Player.GF.1yr',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.RB.rushing.Avg.2nd'
)
d <- d[,cols]
dim(d)
lm(Stat.Result.rushing.Yds ~ ., data=d)
head(d)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.rushing.Yds',
'Stat.Player.rushing.Yds.Adj.None',
'Stat.Player.rushing.Avg.Ratio',
'Attr.Player.GF.1yr',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.RB.rushing.Avg.2nd'
)
d <- d[,cols]
head(d)
lm(Stat.Result.rushing.Yds ~ ., data=d)
lm.res <- lm(Stat.Result.rushing.Yds ~ ., data=d)
summary(lm.res)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.rushing.Yds',
'Stat.Player.rushing.Yds.Adj.None',
'Stat.Player.rushing.Avg.Ratio',
'Attr.Player.GF.1yr',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.RB.rushing.Avg.2nd',
'Stat.Opp.RB.rushing.Yds.2nd'
)
d <- d[,cols]
lm.res <- lm(Stat.Result.rushing.Yds ~ ., data=d)
summary(lm.res)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.rushing.Yds',
'Stat.Player.rushing.Yds.Adj.None',
'Stat.Player.rushing.Avg.Ratio',
'Attr.Player.GS.1yr',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.RB.rushing.Avg.2nd',
'Stat.Opp.RB.rushing.Yds.2nd'
)
d <- d[,cols]
lm.res <- lm(Stat.Result.rushing.Yds ~ ., data=d)
summary(lm.res)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
names(d)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.rushing.Yds',
'Stat.Player.rushing.Yds.Adj.None',
'Stat.Player.rushing.Avg.Ratio',
'Stat.Player.ContribGF',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.RB.rushing.Avg.2nd',
'Stat.Opp.RB.rushing.Yds.2nd'
)
d <- d[,cols]
lm.res <- lm(Stat.Result.rushing.Yds ~ ., data=d)
summary(lm.res)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.passing.Yds',
'Stat.Player.passing.Yds.Adj.None',
'Stat.Player.passing.Yds.Ratio',
'Stat:Player:passing:Yds:Adj:90',
'Stat.Player.ContribGF',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.QB.rushing.Yds.2nd'
)
d <- d[,cols]
names(d)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.passing.Yds',
'Stat.Player.passing.Yds.Adj.None',
'Stat.Player.passing.Yds.Ratio',
'Stat.Player.passing.Yds.Adj.90',
'Stat.Player.ContribGF',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.QB.rushing.Yds.2nd'
)
d <- d[,cols]
lm.res <- lm(Stat.Result.passing.Yds ~ ., data=d)
summary(lm.res)
lm.res <- lm(Stat.Result.passing.Yds ~ Stat.Player.passing.Yds.Adj.None, data=d)
summary(lm.res)
lm.res <- lm(Stat.Result.passing.Yds ~ Stat.Player.passing.Yds.Adj.90, data=d)
summary(lm.res)
lm.res <- lm(Stat.Result.passing.Yds ~ Stat.Player.passing.Yds.Adj.None + Stat.Player.passing.Yds.Ratio, data=d)
summary(lm.res)
ggplot(d, aes(x=Stat.Player.passing.Yds.Ratio, y=Stat.Result.passing.Yds)) + geom_point()
library(ggplot2)
ggplot(d, aes(x=Stat.Player.passing.Yds.Ratio, y=Stat.Result.passing.Yds)) + geom_point()
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.passing.Yds',
'Stat.Result.passing.Yds.Diff'
'Stat.Player.passing.Yds.Adj.None',
'Stat.Player.passing.Yds.Ratio',
'Stat.Player.passing.Yds.Adj.90',
'Stat.Player.ContribGF',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.QB.rushing.Yds.2nd'
)
d <- d[,cols]
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.passing.Yds',
'Stat.Result.passing.Yds.Diff',
'Stat.Player.passing.Yds.Adj.None',
'Stat.Player.passing.Yds.Ratio',
'Stat.Player.passing.Yds.Adj.90',
'Stat.Player.ContribGF',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.QB.rushing.Yds.2nd'
)
d <- d[,cols]
names(d)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.passing.Yds',
'Stat.Result.passing.Yds.Diff',
'Stat.Player.passing.Yds.Adj.None',
'Stat.Player.passing.Yds.Ratio',
'Stat.Player.passing.Yds.Adj.90',
'Stat.Player.ContribGF',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.QB.rushing.Yds.2nd'
)
d <- d[,cols]
names(d)
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.passing.Yds',
'Stat.Player.passing.Yds.Diff',
'Stat.Player.passing.Yds.Adj.None',
'Stat.Player.passing.Yds.Ratio',
'Stat.Player.passing.Yds.Adj.90',
'Stat.Player.ContribGF',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.QB.rushing.Yds.2nd'
)
d <- d[,cols]
ggplot(d, aes(x=Stat.Player.passing.Yds.Diff, y=Stat.Result.passing.Yds)) + geom_point()
d <- read.csv('/tmp/nfl_export.csv', stringsAsFactors=F)
cols <- c(
'Stat.Result.passing.Yds',
'Stat.Player.passing.Yds.Diff',
'Stat.Player.passing.Yds.Adj.None',
#'Stat.Player.passing.Yds.Ratio',
#'Stat.Player.passing.Yds.Adj.90',
'Stat.Player.ContribGF',
'Attr.Weather.Temp',
'Stat.Opp.QB.passing.Yds.2nd',
'Stat.Opp.QB.rushing.Yds.2nd'
)
d <- d[,cols]
lm.res <- lm(Stat.Result.passing.Yds ~ ., data=d)
summary(lm.res)
ggplot(d, aes(x=Stat.Opp.QB.passing.Yds.2nd, y=Stat.Result.passing.Yds)) + geom_point()
qqplot(c(1,2,3,2,4,5), rnorm(100))
d <- read.csv('/tmp/ts.csv')
head(d)
feats <- c('year', 'day_of_year', 'day_oe_dist', 'day_of_week')
resp <- 'value'
d[,feats] <- d %>% select(.dots=feats)
library(dplyr)
d[,feats] <- d %>% select(.dots=feats)
d[,feats] <- d %>% select_(.dots=feats)
d <- read.csv('/tmp/ts.csv')
feats <- c('year', 'day_of_year', 'day_oe_dist', 'day_of_week')
resp <- 'value'
d[,feats] <- d %>% select_(.dots=feats)
head(d)
d <- read.csv('/tmp/ts.csv')
feats <- c('year', 'day_of_year', 'day_oe_dist', 'day_of_week')
resp <- 'value'
standardize <- function(x) (x - mean(x))/sd(x)
d[,feats] <- d %>% select_(.dots=feats) %>% mutate_each(funs(standardize))
head(d)
summary(d)
d.train <- d %>% filter(type == 'training')
install.packages('neuralnet')
library(neuralnet)
res <- neuralnet(value ~ year + day_of_year + day_of_week + day_oe_dist, d.train, hidden=2, rep=5)
d.train <- d %>% filter(type == 'training')
mean.v <- mean(d.train$value)
sd.v <- sd(d.train$value)
d.train$value = standardize(d.train$value)
res <- neuralnet(value ~ year + day_of_year + day_of_week + day_oe_dist, d.train, hidden=2, rep=5)
plot(res, rep="best")
predict(res, subset(d, type == 'validation'))
library(nnet)
?nnet
d <- read.csv('/tmp/ts.csv')
feats <- c('year', 'day_of_year', 'day_oe_dist', 'day_of_week')
resp <- 'value'
standardize <- function(x) (x - mean(x))/sd(x)
d[,feats] <- d %>% dplyr::select_(.dots=feats) %>% mutate_each(funs(standardize))
d.train <- d %>% filter(type == 'training') %>% dplyr::select(-date)
names(d.train)
d <- read.csv('/tmp/ts.csv')
feats <- c('year', 'day_of_year', 'day_oe_dist', 'day_of_week')
resp <- 'value'
standardize <- function(x) (x - mean(x))/sd(x)
d[,feats] <- d %>% dplyr::select_(.dots=feats) %>% mutate_each(funs(standardize))
d.train <- d %>% filter(type == 'training') %>% dplyr::select(-date, -type)
names(d.train)
res <- nnet(value ~ .^3, d.train, size=10)
predict(res, subset(d, type == 'validation'))
subset(d, type == 'validation')
d$type
predict(res, subset(d, type == 'prediction'))
res <- nnet(value ~ .^3, d.train, size=100)
predict(res, subset(d, type == 'prediction'))
res <- nnet(value ~ .^3, d.train, size=100000)
res <- nnet(value ~ .^3, d.train, size=10000)
res <- nnet(value ~ .^3, d.train, size=1000)
res <- nnet(value ~ .^3, d.train, size=100)
res <- nnet(value ~ .*., d.train, size=100)
res <- nnet(value ~ .*., d.train, size=10)
predict(res, subset(d, type == 'prediction'))
?neuralnet
res <- neuralnet(value ~ .^3 , d.train, hidden=100, rep=5)
res <- neuralnet(value ~ .^3 , data=d.train, hidden=100, rep=5)
res <- neuralnet(value ~ . , data=d.train, hidden=100, rep=5)
formula('value ~ .^2')
formula('value ~ .^2', data=d.train)
res <- neuralnet(value ~  year + day_of_year + day_of_week + day_oe_dist, data=d.train, hidden=100, rep=5)
res <- neuralnet(value ~  year + day_of_year + day_of_week + day_oe_dist, data=d.train, hidden=50, rep=5)
res <- neuralnet(value ~  year + day_of_year + day_of_week + day_oe_dist, data=d.train, hidden=10, rep=2)
res <- neuralnet(value ~  year + day_of_year + day_of_week + day_oe_dist,
data=d.train, hidden=4, rep=5)
library(GPfit)
install.packages('GPfit')
library(GPfit)
?GP_fit
GP_fit(d.train %>% dplyr::select(-value), d.train$value)
d <- read.csv('/tmp/ts.csv')
feats <- c('year', 'day_of_year', 'day_oe_dist', 'day_of_week')
resp <- 'value'
#standardize <- function(x) (x - mean(x))/sd(x)
standardize <- function(x) (x - min(x))/(max(x) - min(x))
d[,feats] <- d %>% dplyr::select_(.dots=feats) %>% mutate_each(funs(standardize))
d.train <- d %>% filter(type == 'training') %>% dplyr::select(-date, -type)
summary(d.train)
mean.v <- mean(d.train$value)
sd.v <- sd(d.train$value)
d.train$value = standardize(d.train$value)
fit <- GP_fit(d.train %>% dplyr::select(-value), d.train$value)
qnorm(.5)
qnorm(.05)
qnorm(.005)
qnorm(.01)
d.file <- '/Users/eczech/data/meetups/genomics/data_subset_v2.csv'
d <- read.csv(d.file, sep=',')
d.dist <- dist(d)
library(TDAmapper)
library(fastcluster)
library(foreach)
library(iterators)
eps <- 1000
d.mat <- as.matrix(d)
filter_values <- foreach(i=1:nrow(d.mat)) %do% {
ri <- d.mat[i,]
y <- foreach(j=1:nrow(d.mat), combine=c) %do% {
if (j == i)
return(NULL)
dt <- ri - d.mat[j,]
dt <- (dt %*% dt)[1,1]
exp(-dt/eps)
}
sum(unlist(y))
}
length(filter_values)
m1 <- mapper1D(
distance_matrix = d.dist,
filter_values = unlist(filter_values),
num_intervals = 10,
percent_overlap = 50,
num_bins_when_clustering = 10)
g1 <- graph.adjacency(m1$adjacency, mode="undirected")
plot(g1, layout = layout.auto(g1) )
tkplot(g1)
install.packages("igraph")
library(igraph)
g1 <- graph.adjacency(m1$adjacency, mode="undirected")
plot(g1, layout = layout.auto(g1) )
tkplot(g1)
m1
?graph.adjacency
eps <- 1000
d.mat <- as.matrix(d)
filter_values <- foreach(i=1:nrow(d.mat)) %do% {
ri <- d.mat[i,]
y <- foreach(j=1:nrow(d.mat), combine=c) %do% {
#     if (j == i)
#       return(NULL)
dt <- ri - d.mat[j,]
dt <- (dt %*% dt)[1,1]
exp(-dt/eps)
}
sum(unlist(y))
}
length(filter_values)
m1 <- mapper1D(
distance_matrix = d.dist,
filter_values = unlist(filter_values),
num_intervals = 10,
percent_overlap = 50,
num_bins_when_clustering = 10)
install.packages("igraph")
library(igraph)
g1 <- graph.adjacency(m1$adjacency, mode="undirected")
plot(g1, layout = layout.auto(g1) )
tkplot(g1)
install.packages("igraph")
plan.dir <- GetProjectPath(file.path('POC', 'data', 'plans', 'carefirst'))
# Read in carefirst -> PSA variable mapping
map.file  <- file.path(plan.dir, 'Carefirst PSA Variable Mapping.csv')
map.data <- read.csv(map.file, stringsAsFactors=F)
# Convert data frame into a named vector where the names are the carefirst
# attributes and the values are the PSA variable name
map.data <- map.data$PSA_Variable %>% setNames(map.data$Attribute_Display)
# Manual additions
map.data['Actuarial Range'] = 'ActuarialRange'
map.data['LegalEntity'] = 'LegalEntity'
map.data['Fund Type'] = 'FundType'
map.data['CDHType'] = 'CDHType'
# > map.data[1:3]
#      Deductible Filter           CopayIntValue   DEDUCTIBLE_FAM_FILTER
# "INDIVIDUAL_DEDUCTIBLE"       "DR_OFFICE_VISIT"     "FAMILY_DEDUCTIBLE"
# Choose input file
#attr.file <- file.path(plan.dir, 'Carefirst qMax - Plan Attributes.xml')
#attr.file <- file.path(plan.dir, 'CareFirst-SM-VA-Attributes.xml')
attr.file <- file.path(plan.dir, 'CareFirst-SM-MD-Attributes.xml')
# Parse the xml attributes file and extract objects representing plans
attr.data <- xmlInternalTreeParse(attr.file) %>% getNodeSet("//QuoteProduct")
ParsePSAAttributes <- function(plan){
foreach(plan.attr=getNodeSet(plan, "PlanAttribute"), .combine=rbind) %do% {
carefirst.attr <- xmlGetAttr(plan.attr, 'AttributeType')
if (!carefirst.attr %in% names(map.data))
return(NULL)
psa.attr <- map.data[[carefirst.attr]]
#psa.value <- xmlGetAttr(plan.attr, 'ComparisonAttributeValue')
psa.value <- xmlGetAttr(plan.attr, 'AttributeValue')
psa.value <- if (psa.value == 'N/A') NA else psa.value
# Return the result as a data frame with 2 columns like this:
# attr.name             attr.value
# PD_Flag_OOP           NA
# Copay_Flag_OOP        NA
# PD_OOP_Max            false
# INDIVIDUAL_DEDUCTIBLE 1000
# DR_OFFICE_VISIT       0
data.frame(attr.name=psa.attr, attr.value=psa.value)
}
}
GetMedicalPlanAttributes <- function(plan){
# Fetch the PSA attributes for the plan
plan.attr <- ParsePSAAttributes(plan)
# Pivot attribute data frame so that attribute names become column names
plan.attr <- dcast(plan.attr, 1 ~ attr.name, value.var='attr.value') %>% select(-1)
plan.meta <- data.frame(
plan_name = xmlValue(plan[['ProductName']]),
plan_product_id = xmlValue(plan[['ProductId']]),
plan_product_type = xmlValue(plan[['ProductType']]),
plan_external_id = xmlValue(plan[['PlanExternalId']])
)
# Combine and return the PSA properties with the plan meta data
cbind(plan.meta, plan.attr)
}
plan.data <- foreach(plan=attr.data, .combine=rbind) %do% {
if (xmlValue(plan[['BenefitType']]) != 'MEDICAL')
return(NULL)
GetMedicalPlanAttributes(plan)
}
library(dplyr)
library(foreach)
library(rstan)
library(dplyr)
library(ggplot2)
library(reshape2)
source('~/repos/portfolio/demonstrative/R/pbto2/common.R')
source('~/repos/portfolio/demonstrative/R/pbto2/nonlinear_utils.R')
source('~/repos/portfolio/demonstrative/R/pbto2/sim/data_gen.R')
rstan_options(auto_write=T)
options(mc.cores = parallel::detectCores())
static.features <- c('age', 'marshall', 'gcs', 'sex')
#static.features <- c('age', 'sex')
ts.feature <- 'pbto2'
features <- c(static.features, ts.feature)
d <- read.csv('~/data/pbto2/export/data_stan_input.csv', stringsAsFactors=F)
br <- -6; p <- .3; bc <- 0;
a1 <- br * p; a2 <- (1 - p) * br;
b1 <- 25; b2 <- -20;
c1 <- -.6; c2 <- .55; # set based on quantiles (25/75%)
# Use transformed actual data
# ds <- get.cleaned.data(d, features, scale=T, sample.frac=NULL, outcome.func=gos.to.binom)
# du <- get.cleaned.data(d, features, scale=F, sample.frac=NULL, outcome.func=gos.to.binom)
# unscaled.value <- function(x, var) x * sd(du[,var]) + mean(du[,var])
# dp <- get.sim.data.from.actual(ds)
# d.stan <- dp %>% select(-r1, -r2, -p, -w) %>%
#   mutate(uid=as.integer(factor(uid)))
# Use simulated data
br <- -12; p <- .3; bc <- 0;
a1 <- br * p; a2 <- (1 - p) * br;
b1 <- 25; b2 <- -20;
c1 <- -.6; c2 <- .55; # set based on quantiles (25/75%)
# Use transformed actual data
# ds <- get.cleaned.data(d, features, scale=T, sample.frac=NULL, outcome.func=gos.to.binom)
# du <- get.cleaned.data(d, features, scale=F, sample.frac=NULL, outcome.func=gos.to.binom)
# unscaled.value <- function(x, var) x * sd(du[,var]) + mean(du[,var])
# dp <- get.sim.data.from.actual(ds)
# d.stan <- dp %>% select(-r1, -r2, -p, -w) %>%
#   mutate(uid=as.integer(factor(uid)))
# Use simulated data
sim.data <- get.sim.data(d, a1, a2, b1, b2, c1, c2, n=500, seed=1234)
dp <- sim.data$res %>% mutate_each_(funs(scale), static.features)
v <- sim.data$ts.value.unscaled
x <- seq(min(dp$pbto2)-10, max(dp$pbto2), length.out = 100)
unscaled.value <- function(x) x * sd(v) + mean(v)
d.stan <- dp %>% select(-r1, -r2, -p) %>%
mutate(uid=as.integer(factor(uid)))
# Diagnostics
plot(x, double.logistic(x, a1, a2, b1, b2, c1, c2, bc), type='l')
sapply(quantile(dp$pbto2, probs=c(.1, .25, .5, .75, .99)), function(x) abline(v=x))
dp %>% group_by(uid) %>% summarise(r2=min(r2), o=min(outcome), p=min(p)) %>%
ungroup %>% arrange(r2) %>% melt(id.vars=c('uid', 'o')) %>%
ggplot(aes(x=value, color=factor(o))) + geom_density() + facet_wrap(~variable, scales='free')
d.model <- get.stan.data(d.stan, static.features, ts.feature)
setwd('~/repos/portfolio/demonstrative/R/pbto2/models/stan')
model.file <- 'nonlinear_binom_2.stan'
posterior <- stan(model.file, data = d.model,
warmup = 100, iter = 1000, thin = 10,
chains = 1, verbose = FALSE)
post <- rstan::extract(posterior)
pars <- c('beta', 'betaz', 'a1', 'a2', 'b1', 'b2', 'c', 'alpha', 'p')
print(posterior, pars)
rstan::traceplot(posterior, pars)
y.est <- get.mean.curve(post, x)
y.act <- double.logistic(x, a1, a2, b1, b2, c1, c2)
y.main <- data.frame(i=0, x=unscaled.value(x), y.est, y.act) %>%
melt(id.vars=c('i', 'x'), value.name = 'y')
#y.main %>% ggplot(aes(x=x, y=y, color=variable)) + geom_line()
n = length(post$lp__)
y.samp <- foreach(i=1:n, .combine=rbind) %do% {
#y <- double.logistic(x, post$a1[i], post$a2[i], post$b1[i], post$b2[i], post$c[i, 1], post$c[i, 2])
y <- double.logistic(x, post$a1[i], post$a2[i], post$b1[i], post$b2[i], post$c[i, 1], post$c[i, 2])
a = log(sqrt(sum((y - y.est)^2)))
data.frame(i, x=unscaled.value(x), y, a=a)
} %>% mutate(a=1-scale.minmax(a))
ggplot(NULL) +
geom_line(aes(x=x, y=y, group=variable, color=variable), size=1, data=y.main) +
geom_line(aes(x=x, y=y, group=i, alpha=a), data=y.samp) +
scale_alpha(range = c(.05, .05)) + theme_bw() +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) +
xlab('PbtO2') + ylab('w(PbtO2)') + ggtitle('Timeseries Weight Function')
